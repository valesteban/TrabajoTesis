{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 1. Create DGL Graph -------------- #\n",
    "data_path = \"datasets/DGL_Graph/MYCODEDiGraph/\"\n",
    "# data_path = \"datasets/DGL_Graph/MYCODEDiGraphDegree/\"\n",
    "gnn = GNN(debug=False)\n",
    "gnn.load_dataset( data_path, force_reload=True)\n",
    "gnn.split_dataset_edges(0.6)\n",
    "\n",
    "labels = gnn.dgl_graph.edata[\"Relationship\"].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. Run PageRank for Graph -------------- #\n",
    "N = gnn.dgl_graph.number_of_nodes()\n",
    "DAMP = 0.85\n",
    "K = 10\n",
    "\n",
    "\n",
    "def compute_pagerank(g):\n",
    "    g.ndata[\"pv\"] = torch.ones(N) / N\n",
    "    degrees = g.out_degrees(g.nodes()).type(torch.float32)\n",
    "    for k in range(K):\n",
    "        g.ndata[\"pv\"] = g.ndata[\"pv\"] / degrees\n",
    "        g.update_all(\n",
    "            message_func=fn.copy_u(u=\"pv\", out=\"m\"),\n",
    "            reduce_func=fn.sum(msg=\"m\", out=\"pv\"),\n",
    "        )\n",
    "        g.ndata[\"pv\"]  = (1 - DAMP) / N + DAMP * g.ndata[\"pv\"]\n",
    "    g.ndata[\"pv\"]  = g.ndata[\"pv\"].unsqueeze(1)\n",
    "    return g.ndata[\"pv\"]\n",
    "\n",
    "\n",
    "pv = compute_pagerank(gnn.dgl_graph)\n",
    "gnn.dgl_graph.ndata[\"h\"] = pv  # Inicializa características del nodo\n",
    "\n",
    "print(gnn.dgl_graph.ndata[\"pv\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definir Dataloader y Sampler Training\n",
    "sampler_train = dgl.dataloading.NeighborSampler([2, 2])\n",
    "\n",
    "\n",
    "# Transformar el sampler para predicción de aristas\n",
    "sampler_train = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler_train, \n",
    "    # exclude='reverse_id',  # Excluir aristas inversas\n",
    "    # reverse_eids=reverse_eids,  # Especificar las aristas inversas\n",
    "    # negative_sampler=dgl.dataloading.negative_sampler.Uniform(5)  # Sampler negativo con 5 muestras negativas\n",
    ")\n",
    "\n",
    "train_dataloader = dgl.dataloading.DataLoader(\n",
    "    # The following arguments are specific to DGL's DataLoader.\n",
    "    gnn.dgl_graph,              # The graph\n",
    "    train_nids,         # The node IDs to iterate over in minibatches\n",
    "    sampler_train,            # The neighbor sampler\n",
    "    # The following arguments are inherited from PyTorch DataLoader.\n",
    "    batch_size=1000,    # Batch size\n",
    "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "    drop_last=False,    # Whether to drop the last incomplete batch\n",
    "    # num_workers=0       # Number of sampler processes\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Definir Dataloader y Sampler Evaluación ------------------------------------------------------------------\n",
    "sampler_test = dgl.dataloading.NeighborSampler([2,2])\n",
    "\n",
    "\n",
    "# Transformar el sampler para predicción de aristas\n",
    "sampler_test = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler_test, \n",
    "    # exclude='reverse_id',  # Excluir aristas inversas\n",
    "    # reverse_eids=reverse_eids,  # Especificar las aristas inversas\n",
    "    # negative_sampler=dgl.dataloading.negative_sampler.Uniform(5)  # Sampler negativo con 5 muestras negativas\n",
    ")\n",
    "\n",
    "test_dataloader = dgl.dataloading.DataLoader(\n",
    "    # The following arguments are specific to DGL's DataLoader.\n",
    "    gnn.dgl_graph,              # The graph\n",
    "    test_nids,         # The node IDs to iterate over in minibatches\n",
    "    sampler_test,            # The neighbor sampler\n",
    "    # The following arguments are inherited from PyTorch DataLoader.\n",
    "    batch_size=1000,    # Batch size\n",
    "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "    drop_last=False,    # Whether to drop the last incomplete batch\n",
    "    # num_workers=0       # Number of sampler processes\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # model = GraphSAGE(num_features_in, num_feat_hidden,num_feat_out)\n",
    "model = ModelSAGESample(gnn.dgl_graph.ndata[\"feat\"].size(1), 100, 25)\n",
    "pred = MLPPredictor(25,1)\n",
    "\n",
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "\n",
    "optimizer = torch.optim.Adam( pred.parameters(), lr=0.05)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "train_error = []\n",
    "train_error_per_epoch = []\n",
    "val_error_per_epoch = []\n",
    "train_edges_id = []\n",
    "\n",
    "# Inicializar listas para guardar logits y labels del entrenamiento\n",
    "all_logits_train = []\n",
    "all_labels_train = []\n",
    "\n",
    "# Inicializar listas para guardar logits y labels de la validacion\n",
    "all_logits_val = []\n",
    "all_labels_val = []\n",
    "# Almacenar la importancia de características por epoch\n",
    "feature_importances = []\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    val_loss_epoch=0\n",
    "    num_batches = 0\n",
    "\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_graph, mfgs) in enumerate(tq):\n",
    "    \n",
    "            mfgs = [graph for graph in mfgs]\n",
    "            edge_ids = output_graph.edata[dgl.EID]\n",
    "            inputs = mfgs[-1].srcdata['pv']\n",
    "\n",
    "            m = compute_pagerank(output_graph)\n",
    "            # Hacer que los inputs requieran gradiente\n",
    "            inputs.requires_grad_(True)\n",
    "            # Embeddings de nodos\n",
    "            print(inputs)\n",
    "            logits =  pred(output_graph, m)\n",
    "            \n",
    "            \n",
    "#             train_edges_id.append(edge_ids)\n",
    "\n",
    "#             # Usar los IDs de los bordes para obtener las etiquetas reales\n",
    "#             labels_batch_output = gnn.dgl_graph.edata['Relationship'][edge_ids].float()\n",
    "\n",
    "#             all_logits_train.append(logits.detach().cpu().numpy())\n",
    "#             all_labels_train.append(labels_batch_output.cpu().numpy())\n",
    "#             # print(\"LOGITS\",logits,logits.shape)\n",
    "#             # print(\"LABEL\",mfgs[-1].edata['Relationship'].shape )\n",
    "\n",
    "#             # Calculo pérdida --------------------------------------------------------\n",
    "#             train_loss = F.binary_cross_entropy_with_logits(logits, labels_batch_output)\n",
    "#             train_error.append(train_loss.item())\n",
    "\n",
    "\n",
    "#             val_loss_epoch += train_loss.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#             # backward\n",
    "#             optimizer.zero_grad()\n",
    "#             # train_loss.backward()\n",
    "#             train_loss.backward()\n",
    "\n",
    "#                         # Capturar los gradientes con respecto a los inputs (características)\n",
    "#             grad = inputs.grad  # Este es el gradiente de la pérdida con respecto a los inputs\n",
    "#             feature_importance_batch = grad.abs().mean(dim=0).cpu().numpy()  # Promediar sobre los nodos y las dimensiones de características\n",
    "#             feature_importances.append(feature_importance_batch)\n",
    "#             optimizer.step()\n",
    "\n",
    "#     train_error_per_epoch.append(val_loss_epoch / num_batches)\n",
    "        \n",
    "#     # Validación\n",
    "#     model.eval()\n",
    "#     val_loss_epoch = 0\n",
    "#     num_batches = 0\n",
    "#     with torch.no_grad():\n",
    "#         with tqdm.tqdm(test_dataloader) as tq:\n",
    "#             for step, (input_nodes, output_graph, mfgs) in enumerate(tq):\n",
    "#                 mfgs = [graph for graph in mfgs]\n",
    "#                 edge_ids = output_graph.edata[dgl.EID]\n",
    "#                 inputs = mfgs[0].srcdata['feat']\n",
    "                \n",
    "                \n",
    "#                 # Forward para validación\n",
    "#                 h_val = model(mfgs, inputs)\n",
    "#                 logits_val = pred(output_graph, h_val)\n",
    "\n",
    "#                 # Guardar logits de validacion\n",
    "#                 all_logits_val.append(logits_val.detach().cpu().numpy())\n",
    "\n",
    "#                 # Obtener las etiuetas reales\n",
    "#                 labels_batch_output_val = gnn.dgl_graph.edata['Relationship'][edge_ids].float()\n",
    "#                 all_labels_val.append(labels_batch_output_val.cpu().numpy())\n",
    "            \n",
    "#                 # Calcular pérdida de validación\n",
    "#                 val_loss_batch = F.binary_cross_entropy_with_logits(logits_val, labels_batch_output_val)\n",
    "#                 val_loss_epoch += val_loss_batch.item()\n",
    "#                 num_batches += 1\n",
    "\n",
    "#     # Almacenar el promedio de la pérdida de validación por epoch\n",
    "#     val_error_per_epoch.append(val_loss_epoch / num_batches)\n",
    "\n",
    "\n",
    "#     if epoch % 1 == 0:\n",
    "#         print('In epoch {}, train loss: {},'.format( epoch,train_loss))\n",
    "\n",
    "\n",
    "\n",
    "# # ----------- 5. Definir threashold -------------- \n",
    "# # Concatenar todos los logits y etiquetas al final del entrenamiento\n",
    "# all_logits_train = np.concatenate(all_logits_train, axis=0)\n",
    "# all_labels_train = np.concatenate(all_labels_train, axis=0)\n",
    "# optimal_threshold = plot_roc_curve(all_labels_train, all_logits_train)\n",
    "# print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "          \n",
    "\n",
    "# # ----------- 6. plot the loss and accuracy de training-------------- #\n",
    "# # FIXME: Ess asi por meintras para q no s ecaiga la funcion\n",
    "# print(\"Train Error\",train_error)\n",
    "# # acc_train = train_error\n",
    "# # val_error = train_error\n",
    "# # acc_val = train_error\n",
    "# # model_complexity = [i for i in range(len(train_error))]\n",
    "\n",
    "\n",
    "# # Crear gráfico del error de entrenamiento\n",
    "# plt.figure()  # Crear una nueva figura\n",
    "# plt.plot([i for i in range(len(train_error))],train_error, label='Train Error', color='blue')  # Error de entrenamiento en azul\n",
    "# plt.xlabel('Mini-Batch')\n",
    "# plt.ylabel('Error')\n",
    "# plt.title('Training Error over Epochs')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()  # Mostrar el gráfico de error de entrenamiento\n",
    "\n",
    "\n",
    "# # ----------- 6. Visualización de la Importancia de Características -------------- #\n",
    "# # Convertir la lista de importancias en un array para análisis\n",
    "# # feature_importances = np.array(feature_importances)\n",
    "\n",
    "# # Calcular la importancia promedio a lo largo de todos los batches y epochs\n",
    "# # avg_feature_importance = feature_importances.mean(axis=0)\n",
    "\n",
    "# # plt.figure()\n",
    "# # plt.bar(range(len(avg_feature_importance)), avg_feature_importance)\n",
    "# # plt.xlabel('Feature Index')\n",
    "# # plt.ylabel('Average Importance')\n",
    "# # plt.title('Average Feature Importance over Training')\n",
    "# # # plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
