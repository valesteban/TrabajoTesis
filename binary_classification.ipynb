{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in ./env310/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./env310/lib/python3.10/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: pandas>=1.2 in ./env310/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./env310/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pillow>=8 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./env310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env310/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env310/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./env310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from modules.gnn import GNN\n",
    "from modules.predictors import DotPredictor, MLPPredictor\n",
    "from modules.models import  ModelGraphSAGE, GraphSAGE, GCN, ModelGCN, ModelSAGESample #ModelGraphSAGE,GraphSAGE, GCN,ModelSAGESample,SAGE, ModelGCN\n",
    "from utils import plot_roc_curve, plot_training, calculate_metrics\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Reload modulos automaticamente\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from modules.graph_from_api import Graph_API\n",
    "# from dgl.sampling import pack_traces\n",
    "\n",
    "\n",
    "# from torch.optim import SparseAdam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from modules.graph import Graph\n",
    "# import numpy as np\n",
    "import os\n",
    "# import fnmatch\n",
    "# import tqdm\n",
    "# import networkx as nx\n",
    "\n",
    "# import dgl.function as fn\n",
    "# from dgl.nn import DeepWalk\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mDiGraph_AllFeatures\u001b[0m/  edges.csv  meta.yaml  nodes.csv\n"
     ]
    }
   ],
   "source": [
    "ls data/DGL_Graph/DiGraph_AllFeatures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de DGL: 2.4.0+cu121\n",
      "Versión de PyTorch: 2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "print(\"Versión de DGL:\", dgl.__version__)\n",
    "print(\"Versión de PyTorch:\", torch.__version__)\n",
    "\n",
    "TOR_LABELS_DICT = {'P2P':0, 'C2P': 1,'P2C': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"data/DGL_Graph/DiGraph_AllFeatures/\"\n",
    "# import pandas as pd\n",
    "\n",
    "# # Cargar los archivos CSV\n",
    "# edges = pd.read_csv(\"data/DGL_Graph/DiGraph_AllFeatures/edges.csv\")\n",
    "# nodes = pd.read_csv(\"data/DGL_Graph/DiGraph_AllFeatures/nodes.csv\")\n",
    "\n",
    "# # Obtener los conjuntos de nodos\n",
    "# edge_nodes = set(edges['src_id']).union(set(edges['dst_id']))\n",
    "# node_ids = set(nodes['node_id'])\n",
    "\n",
    "# print(f\"Total de nodos en edges.csv: {len(edge_nodes)}\")\n",
    "# print(f\"Total de nodos en nodes.csv: {len(node_ids)}\")\n",
    "\n",
    "# # Verificar que todos los nodos en edges.csv estén presentes en nodes.csv\n",
    "# missing_nodes = edge_nodes - node_ids\n",
    "# if missing_nodes:\n",
    "#     print(f\"Faltan los siguientes nodos en nodes.csv: {missing_nodes}\")\n",
    "# else:\n",
    "#     print(\"Todos los nodos en edges.csv están presentes en nodes.csv\")\n",
    "\n",
    "# print(f\"Total de nodos: {len(missing_nodes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diccionario de casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = {\n",
    "    # \"CASO_1\": {\n",
    "    #     \"data_path\": \"data/DGL_Graph/DiGraph_AllFeatures/\",\n",
    "    #     \"features_type\": \"AllFeatures\",\n",
    "    #     \"size_train\": 0.8,\n",
    "    #     \"model\": GCN,\n",
    "    #     \"predictor\": DotPredictor,\n",
    "    #     'sampling': None,\n",
    "    #     'description': 'GCN con predictor de Dot'\n",
    "    # },\n",
    "    # \"CASO_2\": {\n",
    "    #     \"data_path\": \"data/DGL_Graph/DiGraph_AllFeatures/\",\n",
    "    #     \"features_type\": \"AllFeatures\",\n",
    "    #     \"size_train\": 0.8,\n",
    "    #     \"model\": GCN,\n",
    "    #     \"predictor\": MLPPredictor,\n",
    "    #     'sampling': None,\n",
    "    #     'description': 'GCN con predictor de MLP'\n",
    "    # },\n",
    "    # \"CASO_3\": {\n",
    "    #     \"data_path\": \"data/DGL_Graph/DiGraph_AllFeatures/\",\n",
    "    #     \"features_type\": \"AllFeatures\",\n",
    "    #     \"size_train\": 0.8,\n",
    "    #     \"model\": GraphSAGE,\n",
    "    #     \"predictor\": DotPredictor,\n",
    "    #     'sampling': None,\n",
    "    #     \"description\": \"GraphSAGE con predictor de Dot\"\n",
    "    # },\n",
    "    # \"CASO_4\": {\n",
    "    #     \"data_path\": \"data/DGL_Graph/DiGraph_AllFeatures/\",\n",
    "    #     \"features_type\": \"AllFeatures\",\n",
    "    #     \"size_train\": 0.8,\n",
    "    #     \"model\": GraphSAGE,\n",
    "    #     \"predictor\": MLPPredictor,\n",
    "    #     'sampling': None,\n",
    "    # },\n",
    "    # \"CASO_5\": {\n",
    "    #     \"data_path\": \"data/DGL_Graph/DiGraph_DegreeFeatures/\",\n",
    "    #     \"features_type\": \"DiGraph_DegreeFeatures\",\n",
    "    #     \"size_train\": 0.8,\n",
    "    #     \"model\": GraphSAGE,\n",
    "    #     \"predictor\": DotPredictor,\n",
    "    #     'sampling': None,\n",
    "    # },\n",
    "    # \"CASO_6\": {\n",
    "    #     \"data_path\": \"data/DGL_Graph/DiGraph_DegreeFeatures/\",\n",
    "    #     \"features_type\": \"DiGraph_DegreeFeatures\",\n",
    "    #     \"size_train\": 0.8,\n",
    "    #     \"model\": GraphSAGE,\n",
    "    #     \"predictor\": MLPPredictor,\n",
    "    #     'sampling': None,\n",
    "    # },\n",
    "    # \"CASO_7\": {\n",
    "    #     \"data_path\": \"data/DGL_Graph/DiGraph_AllFeatures/\",\n",
    "    #     \"features_type\": \"DiGraph_DegreeFeatures\",\n",
    "    #     \"size_train\": 0.8,\n",
    "    #     \"model\": ModelSAGESample,\n",
    "    #     \"predictor\": MLPPredictor,\n",
    "    #     \"sampling\": dgl.dataloading.NeighborSampler,\n",
    "    #     \"description\": \"Sampling con neighbour node sampling y batch normalizationGraphSAGE con predictor de Dot\"\n",
    "    # },\n",
    "    \"CASO_8\": {\n",
    "        \"data_path\": \"data/DGL_Graph/DiGraph_AllFeatures/\",\n",
    "        \"features_type\": \"DiGraph_DegreeFeatures\",\n",
    "        \"size_train\": 0.8,\n",
    "        \"model\": GraphSAGE,\n",
    "        \"predictor\": MLPPredictor,\n",
    "        \"sampling\": dgl.dataloading.ClusterGCNSampler,\n",
    "        \"description\": \"Sampling con neighbour node sampling y batch normalizationGraphSAGE con predictor de Dot\"\n",
    "    },\n",
    "    # \"CASO_8\": {\n",
    "    # TODO: Agregar caso only degree y otros\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_prepare_data(data_path, size_train):\n",
    "    gnn = GNN(debug=True)\n",
    "    gnn.load_dataset(data_path, force_reload=True)\n",
    "    gnn.split_dataset(size_train)\n",
    "    # gnn.split_dtaset_v2()\n",
    "    gnn.dgl_graph.ndata['feat'] = gnn.dgl_graph.ndata['feat'].float() #FIXME:\n",
    "    return gnn\n",
    "\n",
    "\n",
    "def initialize_model(gnn, model, predictor, lr ,h_feats_model, out_feat_model , out_feat_predictor = None):\n",
    "    in_feats = gnn.dgl_graph.ndata['feat'].shape[1]\n",
    "    print('GGGGGGGGGGG')\n",
    "    model = model(in_feats, h_feats_model, out_feat_model)\n",
    "    print('GGGGGGGGGGG')\n",
    "\n",
    "    if predictor == DotPredictor:\n",
    "        pred = predictor()\n",
    "        print('DP!!!!!')\n",
    "    else:\n",
    "        # MLP Predictor\n",
    "        print('MLP!!!!!')\n",
    "        pred = predictor( out_feat_model, out_feat_predictor)\n",
    "    \n",
    "    optimizer = Adam(itertools.chain(model.parameters(), pred.parameters()), lr=lr)\n",
    "    return model, pred, optimizer\n",
    "\n",
    "def create_dataloaders(gnn, sampler_class ,batch_size=1000):\n",
    "\n",
    "    train_mask = gnn.train_mask\n",
    "    test_mask = gnn.test_mask\n",
    "\n",
    "    # Obtener índices de nodos de entrenamiento\n",
    "    train_nids = train_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Obtener índices de nodos de prueba\n",
    "    test_nids = test_mask.nonzero(as_tuple=True)[0]\n",
    "    # Definir Dataloader y Sampler Training\n",
    "    \n",
    "    if sampler_class == dgl.dataloading.NeighborSampler:\n",
    "        print('NeighborSampler Data Loader')\n",
    "\n",
    "        sampler_train = sampler_class([2, 2])\n",
    "\n",
    "        # Transformar el sampler para predicción de aristas\n",
    "        sampler_train = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            sampler_train, \n",
    "            # exclude='reverse_id',  # Excluir aristas inversas\n",
    "            # reverse_eids=reverse_eids,  # Especificar las aristas inversas\n",
    "            # negative_sampler=dgl.dataloading.negative_sampler.Uniform(5)  # Sampler negativo con 5 muestras negativas\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        train_dataloader = dgl.dataloading.DataLoader(\n",
    "            # The following arguments are specific to DGL's DataLoader.\n",
    "            gnn.dgl_graph,              # The graph\n",
    "            train_nids,         # The node IDs to iterate over in minibatches\n",
    "            sampler_train,            # The neighbor sampler\n",
    "            # The following arguments are inherited from PyTorch DataLoader.\n",
    "            batch_size=1000,    # Batch size\n",
    "            shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "            drop_last=False,    # Whether to drop the last incomplete batch\n",
    "            # num_workers=0       # Number of sampler processes\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Definir Dataloader y Sampler Evaluación ------------------------------------------------------------------\n",
    "        sampler_test = sampler_class([2,2])\n",
    "\n",
    "\n",
    "        # Transformar el sampler para predicción de aristas\n",
    "        sampler_test = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            sampler_test, \n",
    "            # exclude='reverse_id',  # Excluir aristas inversas\n",
    "            # reverse_eids=reverse_eids,  # Especificar las aristas inversas\n",
    "            # negative_sampler=dgl.dataloading.negative_sampler.Uniform(5)  # Sampler negativo con 5 muestras negativas\n",
    "        )\n",
    "\n",
    "        test_dataloader = dgl.dataloading.DataLoader(\n",
    "            # The following arguments are specific to DGL's DataLoader.\n",
    "            gnn.dgl_graph,              # The graph\n",
    "            test_nids,         # The node IDs to iterate over in minibatches\n",
    "            sampler_test,            # The neighbor sampler\n",
    "            # The following arguments are inherited from PyTorch DataLoader.\n",
    "            batch_size=1000,    # Batch size\n",
    "            shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "            drop_last=False,    # Whether to drop the last incomplete batch\n",
    "            # num_workers=0       # Number of sampler processes\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif sampler_class == dgl.dataloading.ClusterGCNSampler:\n",
    "        print('ClusterGCNSampler Data Loader')\n",
    "\n",
    "        num_parts = 1000\n",
    "        g = gnn.dgl_graph\n",
    "        sampler = dgl.dataloading.ClusterGCNSampler(g, num_parts)\n",
    "        train_dataloader = dgl.dataloading.DataLoader(\n",
    "            g, \n",
    "            torch.arange(num_parts), \n",
    "            sampler,\n",
    "            batch_size=20, \n",
    "            shuffle=True, \n",
    "            drop_last=False, \n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        test_dataloader = dgl.dataloading.DataLoader(\n",
    "            g, \n",
    "            torch.arange(num_parts), \n",
    "            sampler,\n",
    "            batch_size=20, \n",
    "            shuffle=True, \n",
    "            drop_last=False, \n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        # for subg in dataloader:\n",
    "        #     print(subg)\n",
    "        #     print(\"-------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(case, gnn, model, pred, optimizer, epochs=100):\n",
    "    train_mask = gnn.train_mask\n",
    "    test_mask = gnn.test_mask\n",
    "    labels = gnn.dgl_graph.edata[\"Relationship\"].float()\n",
    "\n",
    "    model_complexity = []\n",
    "    acc_val = []\n",
    "    acc_train = []\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # forward\n",
    "        h = model(gnn.dgl_graph, gnn.dgl_graph.ndata['feat'])\n",
    "        logits = pred(gnn.dgl_graph, h)\n",
    "\n",
    "        # Calculo pérdida\n",
    "        train_loss = F.binary_cross_entropy_with_logits(logits[train_mask], labels[train_mask])\n",
    "        val_loss = F.binary_cross_entropy_with_logits(logits[gnn.val_mask], labels[gnn.val_mask])\n",
    "\n",
    "        # Calculo accuracy\n",
    "        train_acc = logits[train_mask].detach().numpy()\n",
    "        val_acc = logits[gnn.val_mask].detach().numpy()\n",
    "\n",
    "        # Guardar valores\n",
    "        train_error.append(train_loss.item())\n",
    "        val_error.append(val_loss.item())\n",
    "        acc_train.append(train_acc)\n",
    "        acc_val.append(val_acc)\n",
    "        model_complexity.append(epoch + 1)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print('In epoch {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "    # Guardar los valores en un archivo CSV\n",
    "    df = pd.DataFrame({\n",
    "        'epoch': model_complexity,\n",
    "        'train_error': train_error,\n",
    "        'val_error': val_error,\n",
    "        'acc_train': acc_train,\n",
    "        'acc_val': acc_val\n",
    "    })\n",
    "    df.to_csv(f'results/binary_classification/training_results_{case}.csv', index=False)\n",
    "\n",
    "    return logits, train_error, val_error, acc_train, acc_val, model_complexity\n",
    "\n",
    "\n",
    "def train_model_sampler_neighbour(case, gnn, model, pred, optimizer,train_dataloader,test_dataloader, epochs=100):\n",
    "\n",
    "    acc_val = []\n",
    "    acc_train = []\n",
    "\n",
    "    train_error = []\n",
    "    train_error_per_epoch = []\n",
    "    val_error_per_epoch = []\n",
    "    train_edges_id = []\n",
    "\n",
    "    # Inicializar listas para guardar logits y labels del entrenamiento\n",
    "    all_logits_train = []\n",
    "    all_labels_train = []\n",
    "\n",
    "    # Inicializar listas para guardar logits y labels de la validacion\n",
    "    all_logits_val = []\n",
    "    all_labels_val = []\n",
    "    # Almacenar la importancia de características por epoch\n",
    "    feature_importances = []\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        val_loss_epoch=0\n",
    "        num_batches = 0\n",
    "\n",
    "        with tqdm.tqdm(train_dataloader) as tq:\n",
    "            for step, (input_nodes, output_graph, mfgs) in enumerate(tq):\n",
    "        \n",
    "                mfgs = [graph for graph in mfgs]\n",
    "                edge_ids = output_graph.edata[dgl.EID]\n",
    "                inputs = mfgs[0].srcdata['feat']\n",
    "                # Hacer que los inputs requieran gradiente\n",
    "                # Embeddings de nodos\n",
    "                h = model(mfgs, inputs)\n",
    "                logits =  pred(output_graph, h)\n",
    "            \n",
    "                \n",
    "                \n",
    "                train_edges_id.append(edge_ids)\n",
    "\n",
    "                # Usar los IDs de los bordes para obtener las etiquetas reales\n",
    "                labels_batch_output = gnn.dgl_graph.edata['Relationship'][edge_ids].to(torch.int64)\n",
    "                \n",
    "                all_logits_train.append(logits.detach().cpu().numpy())\n",
    "                all_labels_train.append(labels_batch_output.cpu().numpy())\n",
    "\n",
    "                # Calculo pérdida --------------------------------------------------------\n",
    "                train_loss = F.cross_entropy(logits, labels_batch_output)\n",
    "                train_error.append(train_loss.item())\n",
    "\n",
    "                val_loss_epoch += train_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "\n",
    "                # val_loss_epoch += train_loss.item()\n",
    "                # num_batches += 1\n",
    "                \n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "        train_error_per_epoch.append(val_loss_epoch / num_batches)\n",
    "\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        num_batches = 0\n",
    "        with torch.no_grad():\n",
    "            with tqdm.tqdm(test_dataloader) as tq:\n",
    "                for step, (input_nodes, output_graph, mfgs) in enumerate(tq):\n",
    "                    mfgs = [graph for graph in mfgs]\n",
    "                    edge_ids = output_graph.edata[dgl.EID]\n",
    "                    inputs = mfgs[0].srcdata['feat']\n",
    "                    \n",
    "                    \n",
    "                    # Forward para validación\n",
    "                    h_val = model(mfgs, inputs)\n",
    "                    logits_val = pred(output_graph, h_val)\n",
    "\n",
    "                    # Guardar logits de validacion\n",
    "                    all_logits_val.append(logits_val.detach().cpu().numpy())\n",
    "\n",
    "                    # Obtener las etiuetas reales\n",
    "                    labels_batch_output_val = gnn.dgl_graph.edata['Relationship'][edge_ids].to(torch.int64)\n",
    "                    all_labels_val.append(labels_batch_output_val.cpu().numpy())\n",
    "                \n",
    "                    # Calcular pérdida de validación\n",
    "                    val_loss_batch = F.cross_entropy(logits_val, labels_batch_output_val)\n",
    "                    val_loss_epoch += val_loss_batch.item()\n",
    "                    num_batches += 1\n",
    "                # Almacenar el promedio de la pérdida de validación por epoch\n",
    "        val_error_per_epoch.append(val_loss_epoch / num_batches)\n",
    "        if epoch % 1 == 0:\n",
    "            print('In epoch {}, train loss: {},'.format( epoch,train_loss))\n",
    "\n",
    "\n",
    "\n",
    "def train_model_sampler_cluster(case,model, pred, optimizer,train_dataloader,test_dataloader, epochs=100):\n",
    "    # ----------- 4. training -------------------------------- #\n",
    "    acc_val = []\n",
    "    acc_train = []\n",
    "\n",
    "    train_error = []\n",
    "    train_error_per_epoch = []\n",
    "    val_error_per_epoch = []\n",
    "    train_edges_id = []\n",
    "\n",
    "    # Inicializar listas para guardar logits y labels del entrenamiento\n",
    "    all_logits_train = []\n",
    "    all_labels_train = []\n",
    "\n",
    "    # Inicializar listas para guardar logits y labels de la validacion\n",
    "    all_logits_val = []\n",
    "    all_labels_val = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # model.train()\n",
    "        for it, sg in enumerate(train_dataloader):\n",
    "            # print(it,sg)\n",
    "            inputs = sg.ndata['feat'].float()\n",
    "            # print(\"[INPUTS]\",inputs)\n",
    "            labels = sg.edata['Relationship'].float()\n",
    "\n",
    "            train_mask = sg.edata['train_mask'].bool()\n",
    "            # print(\"[TRAIN_MASK]\",train_mask)\n",
    "\n",
    "            h = model(sg, inputs)\n",
    "            logits =  pred(sg, h)\n",
    "\n",
    "            # print(\"[LABELS]\",labels)\n",
    "            all_logits_train.append(logits[train_mask].detach().cpu().numpy())\n",
    "            all_labels_train.append(labels[train_mask].cpu().numpy())\n",
    "            \n",
    "            # logits = model(sg, inputs)\n",
    "            train_loss = F.binary_cross_entropy_with_logits(logits[train_mask], labels[train_mask])\n",
    "            train_error.append(train_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # if epoch % 5000 == 0:\n",
    "            #     print(f\"Loss {train_loss}\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds, test_preds = [], []\n",
    "                val_labels, test_labels = [], []\n",
    "                for it, sg in enumerate(test_dataloader):\n",
    "                    inputs = sg.ndata[\"feat\"]\n",
    "                    labels = sg.edata[\"Relationship\"]\n",
    "\n",
    "                    val_mask = sg.edata[\"val_mask\"].bool()\n",
    "                    test_mask = sg.edata[\"test_mask\"].bool()\n",
    "\n",
    "                    h = model(sg, inputs)\n",
    "                    logits =  pred(sg, h)\n",
    "\n",
    "                    val_preds.append(logits[val_mask])\n",
    "                    val_labels.append(labels[val_mask])\n",
    "                    test_preds.append(logits[test_mask])\n",
    "                    test_labels.append(labels[test_mask])\n",
    "\n",
    "                val_preds = torch.cat(val_preds, 0)\n",
    "                val_labels = torch.cat(val_labels, 0)\n",
    "                test_preds = torch.cat(test_preds, 0)\n",
    "                test_labels = torch.cat(test_labels, 0)\n",
    "\n",
    "                all_logits_val.append(val_preds)\n",
    "                all_labels_val.append(val_labels)\n",
    "\n",
    "    # Concatenar todos los logits y etiquetas al final del entrenamiento\n",
    "    all_logits_train = np.concatenate(all_logits_train, axis=0)\n",
    "    all_labels_train = np.concatenate(all_labels_train, axis=0)\n",
    "    all_logits_val = np.concatenate(all_logits_val, axis=0)\n",
    "    all_labels_val = np.concatenate(all_labels_val, axis=0)\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    os.makedirs('results/binary_classification', exist_ok=True)\n",
    "\n",
    "    # Guardar los valores en archivos CSV\n",
    "    df_train = pd.DataFrame({\n",
    "        'logits': all_logits_train.flatten(),\n",
    "        'labels': all_labels_train.flatten()\n",
    "    })\n",
    "    df_train.to_csv(f'results/binary_classification/logits_labels_train_{case}.csv', index=False)\n",
    "\n",
    "    df_val = pd.DataFrame({\n",
    "        'logits': all_logits_val.flatten(),\n",
    "        'labels': all_labels_val.flatten()\n",
    "    })\n",
    "    df_val.to_csv(f'results/binary_classification/logits_labels_val_{case}.csv', index=False)\n",
    "\n",
    "\n",
    "    return all_logits_train, all_labels_train, train_error, all_logits_val, all_labels_val\n",
    "def evaluate_model(gnn, all_logits_train, train_mask_or_labels ,sampling = None):\n",
    "    \n",
    "    if not sampling:\n",
    "        train_mask = train_mask_or_labels\n",
    "        true_train_labels = gnn.dgl_graph.edata[\"Relationship\"][train_mask]\n",
    "        predicted_train_scores = all_logits_train.detach().numpy()[train_mask]\n",
    "    else:\n",
    "        # Caso sampling\n",
    "        print('Evaluando modelo con sampling')\n",
    "        print('train_mask_or_labels',len(train_mask_or_labels))\n",
    "        print('all_logits_train',len(all_logits_train))\n",
    "        true_train_labels = train_mask_or_labels\n",
    "        predicted_train_scores = all_logits_train\n",
    "\n",
    "    optimal_threshold = plot_roc_curve(true_train_labels, predicted_train_scores)\n",
    "    return optimal_threshold\n",
    "\n",
    "def plot_error_over_epoch(train_error, train_error_per_epoch, val_error_per_epoch):\n",
    "    # Crear gráfico del error de entrenamiento\n",
    "    plt.figure()  # Crear una nueva figura\n",
    "    plt.plot([i for i in range(len(train_error))],train_error, label='Train Error', color='blue')  # Error de entrenamiento en azul\n",
    "    plt.xlabel('Mini-Batch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Training Error over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()  # Mostrar el gráfico de error de entrenamiento\n",
    "\n",
    "\n",
    "    # Crear gráfico combinado de error de entrenamiento y validación\n",
    "    plt.figure()  # Crear una nueva figura\n",
    "    plt.plot(train_error_per_epoch, label='Train Error', color='blue')  # Error de entrenamiento en azul\n",
    "    plt.plot(val_error_per_epoch, label='Validation Error', color='orange')  # Error de validación en naranja\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Training and Validation Error over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()  # Mostrar el gráfico combinado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CASO_8]\n",
      "Done saving data into cached files.\n",
      "Graph(num_nodes=74145, num_edges=461889,\n",
      "      ndata_schemes={'feat': Scheme(shape=(72,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.int64)})\n",
      "Training edges: 369736\n",
      "Validation edges: 46040\n",
      "Test edges: 46113\n",
      "ClusterGCNSampler Data Loader\n",
      "train_dataloader <dgl.dataloading.dataloader.DataLoader object at 0x761dbcfb9390>\n",
      "test_dataloader <dgl.dataloading.dataloader.DataLoader object at 0x761dbcfb9ab0>\n",
      "MLP\n",
      "GGGGGGGGGGG\n",
      "GGGGGGGGGGG\n",
      "MLP!!!!!\n",
      "CASO CLUSTER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentina/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/dgl/dataloading/dataloader.py:1144: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n",
      "  dgl_warning(\n"
     ]
    }
   ],
   "source": [
    "for case_name, case in cases.items():\n",
    "    print(f\"[{case_name}]\")\n",
    "\n",
    "    data_path = case[\"data_path\"]\n",
    "    size_train = case[\"size_train\"]\n",
    "\n",
    "    # ----------- 1. Cargar grafo y preparar datos -------------- #\n",
    "    gnn = load_and_prepare_data(data_path, size_train)\n",
    "\n",
    "    if case['sampling']: \n",
    "            sampler_class = case['sampling']\n",
    "            train_dataloader, test_dataloader = create_dataloaders(gnn,sampler_class,batch_size=1000)\n",
    "\n",
    "            print('train_dataloader',train_dataloader)\n",
    "            print('test_dataloader',test_dataloader)\n",
    "    # ----------- 2. Inicializa modelo -------------------------- #\n",
    "    model = case[\"model\"]\n",
    "    predictor = case[\"predictor\"]\n",
    "    lr = 0.01\n",
    "    if predictor == DotPredictor:\n",
    "        print(\"DotPredictor\")\n",
    "        h_feats_model = 16\n",
    "        out_feat_model = 2 # num clases\n",
    "        model, pred, optimizer = initialize_model(gnn, model, predictor, lr, h_feats_model, out_feat_model)\n",
    "    else:\n",
    "        print(\"MLP\")\n",
    "        h_feats_model = 16\n",
    "        out_feat_model = 32\n",
    "        out_feat_predictor = 1 # num clases\n",
    "        model, pred, optimizer = initialize_model(gnn, model, predictor, lr ,h_feats_model, out_feat_model , out_feat_predictor)\n",
    "\n",
    "\n",
    "    # ----------- 3. Entrenar modelo ----------------------------- #\n",
    "    epochs = 2\n",
    "    \n",
    "    # if case['sampling'] == dgl.dataloading.NeighborSampler:\n",
    "    #     all_logits_train, all_labels_train, train_error_per_epoch, val_error_per_epoch,train_error,all_logits_val,all_labels_val =train_model_sampler_neighbour(case_name, gnn, model, pred, optimizer,train_dataloader,test_dataloader, epochs)\n",
    "    \n",
    "    # elif case['sampling'] == dgl.dataloading.ClusterGCNSampler:\n",
    "    #     all_logits_train, all_labels_train, train_error, all_logits_val, all_labels_val = train_model_sampler_cluster(case_name,model, pred, optimizer,train_dataloader,test_dataloader, epochs=100)\n",
    "\n",
    "    # else:\n",
    "    #     all_logits_train, train_error, val_error, acc_train, acc_val, model_complexity = train_model(case_name, gnn, model, pred, optimizer, epochs)\n",
    "\n",
    "    if case['sampling'] == dgl.dataloading.NeighborSampler:\n",
    "        print('CASO NEIGHBOUR')\n",
    "        train_model_sampler_neighbour(case_name, gnn, model, pred, optimizer,train_dataloader,test_dataloader, epochs)\n",
    "    \n",
    "    elif case['sampling'] == dgl.dataloading.ClusterGCNSampler:\n",
    "        print('CASO CLUSTER')\n",
    "        train_model_sampler_cluster(case_name,model, pred, optimizer,train_dataloader,test_dataloader, epochs)\n",
    "    \n",
    "\n",
    "    # # ----------- 4. Definir threashold ----------------------------- #\n",
    "    \n",
    "    # if case['sampling']:\n",
    "    #     optimal_threshold = evaluate_model(gnn, all_logits_train, all_labels_train,case['sampling'])\n",
    "    #     plot_error_over_epoch(train_error, train_error_per_epoch, val_error_per_epoch)\n",
    "    # else:\n",
    "    #     optimal_threshold = evaluate_model(gnn, all_logits_train, gnn.train_mask,case['sampling'])\n",
    "\n",
    "    # print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "\n",
    "    # # ----------- 5. Plotear resultados ----------------------------- #\n",
    "    # if  case['sampling']:\n",
    "    #     # FIXME: ver que son los resultados\n",
    "    #     calculate_metrics(all_logits_val,all_labels_val , optimal_threshold)\n",
    "    # else:\n",
    "    #     plot_training(case_name,gnn,train_error_per_epoch,acc_train,val_error_per_epoch,acc_val,model_complexity,optimal_threshold)\n",
    "    # # plot_training(case_name,gnn,train_error,acc_train,val_error,acc_val,model_complexity,optimal_threshold)\n",
    "\n",
    "    # # print(\"------------------------------------------------------\")\n",
    "    # # print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
