{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in ./env310/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in ./env310/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in ./env310/lib/python3.10/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./env310/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./env310/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: scikit-learn in ./env310/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./env310/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./env310/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env310/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env310/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy in ./env310/lib/python3.10/site-packages (1.26.4)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.3.0 in ./env310/lib/python3.10/site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: torchvision==0.18.0 in ./env310/lib/python3.10/site-packages (0.18.0+cu121)\n",
      "Requirement already satisfied: torchaudio==2.3.0 in ./env310/lib/python3.10/site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: networkx in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (11.0.2.54)\n",
      "Requirement already satisfied: sympy in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.3.0 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (3.0.3)\n",
      "Requirement already satisfied: fsspec in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env310/lib/python3.10/site-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./env310/lib/python3.10/site-packages (from torchvision==0.18.0) (11.0.0)\n",
      "Requirement already satisfied: numpy in ./env310/lib/python3.10/site-packages (from torchvision==0.18.0) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env310/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env310/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: tensorflow==2.17.0 in ./env310/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.26.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.71.0)\n",
      "Requirement already satisfied: packaging in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (24.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.0.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (4.25.6)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (25.2.10)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.32.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.17.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.4.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.13.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.9.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (18.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.17.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (4.12.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.2.2)\n",
      "Requirement already satisfied: setuptools in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (59.6.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./env310/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./env310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.45.1)\n",
      "Requirement already satisfied: rich in ./env310/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (14.0.0)\n",
      "Requirement already satisfied: namex in ./env310/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
      "Requirement already satisfied: optree in ./env310/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env310/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./env310/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./env310/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./env310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./env310/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./env310/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./env310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\n",
      "Requirement already satisfied: matplotlib in ./env310/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./env310/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./env310/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./env310/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env310/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./env310/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in ./env310/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./env310/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env310/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: numpy>=1.23 in ./env310/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in ./env310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall gensim -y \n",
    "!pip install gensim #==3.8.3 #4.3.3\n",
    "!pip install scikit-learn\n",
    "!pip install numpy\n",
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip uninstall tensorflow -y\n",
    "!pip install tensorflow==2.17.0\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modules.gnn_models import MLPPredictor\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOR_ORIG_LABELS_DICT = {'P2P':0, 'C2P': 1,'Siblings': 2, 'P2C': 3}\n",
    "TOR_CSV_LABELS_DICT = {'P2P':0,'P2C': -1}\n",
    "DATA_PATH = 'data/CAIDA_AS_Relationships/Serial_1/'\n",
    "PATH = \"/home/valentina/Desktop/GIT/BenchmarckASRelationships/\"\n",
    "class_names = ['P2P', 'C2P', 'P2C']\n",
    "DATA = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar ToR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_training.shape: (795638, 2)\n",
      "y_training.shape: (795638,)\n",
      "x_test.shape: (198910, 2)\n",
      "y_test.shape: (198910,)\n",
      "x_training: [[ 59675  18106]\n",
      " [ 11232   6939]\n",
      " [201806 199524]\n",
      " ...\n",
      " [ 38522 209102]\n",
      " [270362 271253]\n",
      " [ 15305 395846]]\n",
      "y_training: [0 1 0 ... 0 0 2]\n",
      "x_test: [[263152  53180]\n",
      " [  3214  45758]\n",
      " [ 34019   8529]\n",
      " ...\n",
      " [ 37658  16552]\n",
      " [  3356   9930]\n",
      " [138064  59605]]\n",
      "y_test: [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Cargar x_training y x_test\n",
    "x_training = np.load(PATH + DATA + \"x_training.npy\").astype(int)\n",
    "x_test = np.load(PATH + DATA + \"x_test.npy\").astype(int)\n",
    "\n",
    "\n",
    "# Cargar y_training y y_test\n",
    "y_training = np.load(PATH + DATA + \"y_training.npy\").astype(int)\n",
    "y_test = np.load(PATH + DATA + \"y_test.npy\").astype(int)\n",
    "\n",
    "\n",
    "print(\"x_training.shape:\", x_training.shape)\n",
    "print(\"y_training.shape:\", y_training.shape)\n",
    "print(\"x_test.shape:\", x_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n",
    "print('x_training:',x_training)\n",
    "print('y_training:',y_training)\n",
    "print('x_test:',x_test)\n",
    "print('y_test:',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TOTAL ASN]: 74140\n",
      "[EMBEDDINGS LEN]: 16\n",
      "[EMBEDDINGS] tensor([[ 2.9606, -0.5309, -0.0631,  ...,  5.7811,  2.5728,  0.4085],\n",
      "        [ 3.4744, -0.5007,  0.5984,  ...,  5.2775,  2.6044, -0.0717],\n",
      "        [ 3.6919, -0.6067, -1.1352,  ...,  6.2019,  2.6469,  0.9469],\n",
      "        ...,\n",
      "        [ 4.6999,  1.7849, -6.6735,  ...,  6.5724,  1.7841,  2.3854],\n",
      "        [ 5.0911,  0.4358, -3.0184,  ...,  5.7703,  1.8840, -0.7117],\n",
      "        [-0.5820, -0.7045, -0.8149,  ...,  2.3244, -0.5679,  1.2785]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Cargar el tensor de embeddings\n",
    "embeddings_pt = torch.load('embeddings.pt')  # asegúrate que sea un tensor\n",
    "total_ASNs, embedding_vecor_length = embeddings_pt.shape\n",
    "embeddings_np = embeddings_pt.detach().numpy()\n",
    "\n",
    "print(f\"[TOTAL ASN]: {total_ASNs}\" )\n",
    "print(f\"[EMBEDDINGS LEN]: {embedding_vecor_length}\")\n",
    "print(f\"[EMBEDDINGS] {embeddings_pt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "los modelos de embeddings trabajan con índices, no con ASNs reales.\n",
    "\n",
    "En este caso como creamos los embeddings a partir del grafo DGL en el archivo train_embeddings, sabemos que se guardan todos los embeddings desde 0 hasta el mayor ASN, por lo talto sabemos que la matriz guardada en la posicion de las filas coincidira con su ASN. (porque asociamos el numero de nodo con su ASN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_to_index = {}\n",
    "for  i, asn in enumerate(embeddings_np):\n",
    "    asn_to_index[i] = i\n",
    "# FIXME: Esto no tiene sentido ... pero necesito esto para fiotrar mas\n",
    "\n",
    "# print(f\"[ASN TO INDEX]: {asn_to_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora en ves de asocian que ASN_a con ASN_b estan conectados tendromos\n",
    "que el indice_ASN_b esta conectado con indice_ASN_b?c en los datos de training y test de ToR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASN a índice -> x_training y x_test\n",
    "x_training_mapped = []\n",
    "y_training_filtered = []\n",
    "\n",
    "# Recorrer par (asn, asn) y agregamos válidos\n",
    "for (a, b), y in zip(x_training, y_training):\n",
    "    a_index = asn_to_index.get(str(a), 0)\n",
    "    b_index = asn_to_index.get(str(b), 0)\n",
    "\n",
    "    # Ignorar si índice es 0 \n",
    "    if a_index == 0 or b_index == 0 :\n",
    "        continue\n",
    "    \n",
    "    x_training_mapped.append([a_index, b_index])\n",
    "    y_training_filtered.append(y)\n",
    "\n",
    "# Convertir listas a arrays de NumPy\n",
    "x_training_mapped = np.array(x_training_mapped)\n",
    "y_training_filtered = np.array(y_training_filtered)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "x_test_mapped = []\n",
    "y_test_filtered = []\n",
    "\n",
    "for (a, b), y in zip(x_test, y_test):\n",
    "    a_index = asn_to_index.get(str(a), 0)\n",
    "    b_index = asn_to_index.get(str(b), 0)\n",
    "\n",
    "    # Ignorar si índice es 0 \n",
    "    if a_index == 0 or b_index == 0:\n",
    "        continue\n",
    "    \n",
    "    x_test_mapped.append([a_index, b_index])\n",
    "    y_test_filtered.append(y)\n",
    "\n",
    "x_test_mapped = np.array(x_test_mapped)\n",
    "y_test_filtered = np.array(y_test_filtered)\n",
    "\n",
    "# Guardar los datos corregidos\n",
    "np.save(DATA + \"x_training_mapped.npy\", x_training_mapped)\n",
    "np.save(DATA + \"y_training_filtered.npy\", y_training_filtered)\n",
    "np.save(DATA + \"x_test_mapped.npy\", x_test_mapped)\n",
    "np.save(DATA + \"y_test_filtered.npy\", y_test_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo NN de BGP2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, Flatten\n",
    "# # from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "# # from keras.preprocessing import sequence\n",
    "# # from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAPE EMDEDINGS] torch.Size([74140, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"[SHAPE EMDEDINGS]\",embeddings_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentina/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configuración de la RNN BGP2VEC\n",
    "experiment = None\n",
    "num_classes = len(class_names)\n",
    "embedding_trainable = False\n",
    "input_length = 2  # longitud de la secuencia de entrada\n",
    "embedding_vector_length = embeddings_pt.shape[1]  # tamaño de los vectores de embedding\n",
    "total_ASNs = embeddings_pt.shape[0] #42865 # Tamaño del vocabulario\n",
    "\n",
    "\n",
    "# Registro de parámetros del experimento si es aplicable\n",
    "if experiment is not None:\n",
    "    experiment.log_parameter(\"embedding_trainable\", embedding_trainable)\n",
    "\n",
    "# Definición del modelo\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_ASNs, embedding_vector_length, input_length=input_length,\n",
    "                    weights=[embeddings_np], trainable=embedding_trainable))  # Asegúrate de que 'embeddings' esté definida\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model.add(Reshape((32, 2)))  # Cambia Reshape para reflejar las dimensiones correctas\n",
    "#model.add(Reshape((model.output_shape[2], model.output_shape[1])))  # Cambia Reshape para reflejar las dimensiones correctas\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Reshape((32,16)))  # Cambia Reshape para reflejar las dimensiones correctas\n",
    "#model.add(Reshape((model.output_shape[2], model.output_shape[1])))  # Cambia Reshape para reflejar las dimensiones correctas\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,186,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m1,186,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,186,240</span> (4.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,186,240\u001b[0m (4.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,186,240</span> (4.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,186,240\u001b[0m (4.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compilación del modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Y TRAINING VECTOR] []\n",
      "[X TRAINING] [[ 59675  18106]\n",
      " [ 11232   6939]\n",
      " [201806 199524]\n",
      " ...\n",
      " [ 38522 209102]\n",
      " [270362 271253]\n",
      " [ 15305 395846]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convertir las clases a un one-hot vector\n",
    "y_training_vector = to_categorical(y_training_filtered, num_classes) \n",
    "y_test_vector = to_categorical(y_test_filtered, num_classes)\n",
    "print(\"[Y TRAINING VECTOR]\",y_training_vector)\n",
    "print(\"[X TRAINING]\",x_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['P2P', 'C2P', 'P2C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use class_weight to deal with unbalanced dataset\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', list(range(num_classes)), y_training)\n",
    "# print(class_weights)\n",
    "class_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "import os\n",
    "MODEL_NAME = \"NN_GNN\"\n",
    "checkpointer_acc = ModelCheckpoint(monitor='val_acc', filepath= os.getcwd() + MODEL_NAME + '_acc.weights.h5',\n",
    "                                    verbose=1, \n",
    "                                    save_best_only=True, \n",
    "                                    save_weights_only=True)\n",
    "\n",
    "checkpointer_loss = ModelCheckpoint(filepath= os.getcwd() + MODEL_NAME + '_loss.weights.h5', \n",
    "                                    verbose=1, \n",
    "                                    save_best_only=True, \n",
    "                                    save_weights_only=True)\n",
    "\n",
    "callbacks = [checkpointer_loss,checkpointer_acc] #tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "experiment = None\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "steps_per_epoch = math.ceil(len(x_training)/batch_size)\n",
    "val_batch_size = 64\n",
    "validation_steps = math.ceil(len(x_test)/val_batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def generator(features, labels, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(features), batch_size):\n",
    "            batch_features = features[i:i + batch_size]\n",
    "            batch_labels = labels[i:i + batch_size]\n",
    "\n",
    "            # Evitar que devuelva batches incompletos\n",
    "            if len(batch_features) != batch_size:\n",
    "                continue  \n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)\n",
    "\n",
    "\n",
    "def val_generator(features, labels, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(features), batch_size):\n",
    "            batch_features = features[i:i + batch_size]\n",
    "            batch_labels = labels[i:i + batch_size]\n",
    "\n",
    "            if len(batch_features) != batch_size:\n",
    "                continue  \n",
    "            \n",
    "            yield np.array(batch_features), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_training_mapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_training_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_mapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:332\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    325\u001b[0m     (\n\u001b[1;32m    326\u001b[0m         val_x,\n\u001b[1;32m    327\u001b[0m         val_y,\n\u001b[1;32m    328\u001b[0m         val_sample_weight,\n\u001b[1;32m    329\u001b[0m     ) \u001b[38;5;241m=\u001b[39m data_adapter_utils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(validation_data)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Create an iterator that yields batches for one epoch.\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_symbolic_build(iterator\u001b[38;5;241m=\u001b[39mepoch_iterator)\n\u001b[1;32m    345\u001b[0m epoch_iterator\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:718\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, distribute_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m    720\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_adapter\u001b[38;5;241m.\u001b[39mget_tf_dataset()\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:65\u001b[0m, in \u001b[0;36mEpochIterator.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight, steps_per_execution)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_adapter \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_adapter\u001b[38;5;241m.\u001b[39mnum_batches\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/trainers/data_adapters/__init__.py:117\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m class_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `class_weight` is not supported for Python \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator inputs. Received: class_weight=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_weight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m         )\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneratorDataAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/trainers/data_adapters/generator_data_adapter.py:12\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[0;34m(self, generator)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, generator):\n\u001b[0;32m---> 12\u001b[0m     first_batches, generator \u001b[38;5;241m=\u001b[39m \u001b[43mpeek_and_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m generator\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_batches \u001b[38;5;241m=\u001b[39m first_batches\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/keras/src/trainers/data_adapters/generator_data_adapter.py:82\u001b[0m, in \u001b[0;36mpeek_and_restore\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpeek_and_restore\u001b[39m(generator):\n\u001b[0;32m---> 82\u001b[0m     batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_adapter_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_BATCHES_FOR_TENSOR_SPEC\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batches, \u001b[38;5;28;01mlambda\u001b[39;00m: itertools\u001b[38;5;241m.\u001b[39mchain(batches, generator)\n",
      "Cell \u001b[0;32mIn[81], line 25\u001b[0m, in \u001b[0;36mval_generator\u001b[0;34m(features, labels, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mval_generator\u001b[39m(features, labels, batch_size):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(features), batch_size):\n\u001b[1;32m     27\u001b[0m             batch_features \u001b[38;5;241m=\u001b[39m features[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        x=val_generator(x_training_mapped, y_training_vector, batch_size),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        validation_data=val_generator(x_test_mapped, y_test_vector, val_batch_size),\n",
    "        validation_steps=validation_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=val_generator(x_training_mapped, y_training_vector, batch_size),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=val_generator(x_test_mapped, y_test_vector, val_batch_size),\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1  # <-- Agrega esto\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(x_test_mapped, y_test_vector, batch_size=val_batch_size, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (test_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_prediction = model.predict_classes(x_test, batch_size=val_batch_size, verbose=1)\n",
    "y_test_predictions_prob = model.predict(x_test_mapped, batch_size=val_batch_size, verbose=1)\n",
    "# Obtener pradicciones de las clases\n",
    "y_test_prediction = np.argmax(y_test_predictions_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_training_prediction = model.predict_classes(x_training, batch_size=val_batch_size, verbose=1)\n",
    "y_training_predictions_prob = model.predict(x_training_mapped, batch_size=val_batch_size, verbose=1)\n",
    "# Obtener las clases predichas\n",
    "y_training_prediction = np.argmax(y_training_predictions_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los resultados en bgp2vec_results_tor.txt\n",
    "\n",
    "with open(\"bgp2vec_results_tor.txt\", \"w\") as f:\n",
    "    # AS1 | AS2 | Predicción | Real\n",
    "    for i in range(len(y_test_prediction)):\n",
    "        f.write(f\"{x_test_mapped[i][0]}|{x_test_mapped[i][1]}|{y_test_prediction[i]}|{y_test[i]} \\n\")\n",
    "\n",
    "print(\"Archivo bgp2vec_results_tor.txt guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot History Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "\n",
    "with open(os.getcwd() + \"/results/results_\" + MODEL_NAME +  \"_accuracy.pkl\", 'wb') as output:\n",
    "    pickle.dump(history.history, output, pickle.HIGHEST_PROTOCOL)\n",
    "    if experiment is not None:\n",
    "        experiment.log_asset(os.getcwd() + \"/results/results_\" + MODEL_NAME +  \"_accuracy.pkl\")\n",
    "    \n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "x = np.asarray(range(1,epochs + 1))\n",
    "# summarize history for accuracy\n",
    "plt.figure()\n",
    "plt.plot(x, smooth([y*100 for y in history.history['accuracy']],2))\n",
    "# plt.plot(x, [y*100 for y in history_history['val_acc']])\n",
    "plt.plot(x, smooth([y*100 for y in history.history['val_accuracy']],2))\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim(50,100) ###########################\n",
    "plt.legend(['Training', 'Test'], loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and save a confusion matrix for results over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          fname='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "        plt.imshow([[100*j for j in i] for i in cm], interpolation='nearest', cmap=cmap)\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.ax.set_yticklabels(['0%','20%','40%','60%','80%','100%'])\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.1f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, format(cm[i, j]*100, fmt) + '%',\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_filtered, y_test_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized Confusion Msatrix')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
