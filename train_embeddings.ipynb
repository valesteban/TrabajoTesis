{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Embeddings con GNN\n",
    "Aca crearemos embeddings paa un grafo de Internet, es decir represenaciones de los SA a partir de la topologia y atributos de los SA.\n",
    "\n",
    "Para esto tomamos 3 enfoques:\n",
    "\n",
    "\n",
    "*   [Caso 1] Reconstruction Approach autoencoder: A traves de la prediccion de aristas construimos los embeddings de los nodos.\n",
    "*   [Caso 1.1] Reconstruction Approach autoencoder: en ves de ocupar las bgp routes recolectadas de los ribs ocupamos las relaciones de AS rank y le damso attr a las aristas (es solo para probar que lo estemos ahciendo bien y las cosas tienen sentido)\n",
    "*   [Caso 2]: Reconstruction Approach attribute Masking:\n",
    "*   Caso 3: Task Generation Pre-calculated descriptor\n",
    "\n",
    "Para cada uno crearemos un ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from modules.gnn import GNN\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from modules.graph import Graph, create_files\n",
    "from modules.gnn import GNN\n",
    "from modules.gnn_models import GCN, GraphSAGE\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las rutas de los archivos\n",
    "base_path = os.getcwd() + \"/data/\"\n",
    "relationships_file = base_path + \"CAIDA_AS_Relationships/Serial_1/20220701.as-rel.txt.bz2\"\n",
    "features_file = base_path + \"/node_features.csv\"\n",
    "rib_path = base_path + 'sanitized_rib.txt'\n",
    "dataset_graph_path = base_path + 'dgl_graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion Grafo \n",
    "Creamos un grafo nx y dgl, ademas de los archivos edges.csv y nodes.csv a partir de archivos ribs previamente creados o de archivo CAIDA AS Relationships.\n",
    "\n",
    "Crear esos archivos una unica vez con create_graph() una ves ya creados los archivos edges.csv y nodes.csv puedo ocupar directamente la funcion \n",
    "\n",
    "Se le puede indicar el maximo de bgp paths que se quiere (hehco para cuando se leen ribs no de caida) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASO 1: RIBs\n",
    "* Creacion de grafo a partir de paths recolectados de las RIBs por BGPStream\n",
    "* Por ahora le asignamos a todos los nodos embeddings iniciales de de dimension 32 parte con puros 1s todos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaRPETA CREADA]:  /home/valentina/Desktop/GIT/TrabajoTesis/data/dgl_graph/\n",
      "[ARCHIVO EDGES.CSV CREADO]\n",
      "[NX Graph]:  DiGraph with 36173 nodes and 75125 edges\n",
      "[TOPOLOGIA CREADA]\n",
      "[SAVE IN: /home/valentina/Desktop/GIT/TrabajoTesis/data/dgl_graph/nodes.csv]\n",
      "[FEATURES CREADOS]\n",
      "[META CREADO]\n",
      "[NX Graph]:  DiGraph with 14475 nodes and 53427 edges\n",
      "[NX Graph]:  DiGraph with 13822 nodes and 52774 edges\n",
      "[NX Graph]:  DiGraph with 13804 nodes and 52756 edges\n",
      "[NX Graph] DiGraph with 13804 nodes and 52756 edges\n"
     ]
    }
   ],
   "source": [
    "# Definimos las listas de features\n",
    "\n",
    "LIST_FEATURES_NO_CATEG = ['ASN', 'AS_rank_numberAsns', 'AS_rank_numberPrefixes', 'AS_rank_numberAddresses', 'AS_rank_total', 'AS_rank_customer',\n",
    "                 'AS_rank_peer', 'AS_rank_provider', 'peeringDB_ix_count', 'peeringDB_fac_count', 'AS_hegemony', 'cti_top', 'cti_origin']\n",
    "\n",
    "LIST_FEATURES_CATEG = ['AS_rank_continent',\n",
    "                        'peeringDB_info_ratio',\n",
    "                        'peeringDB_info_scope',\n",
    "                        'peeringDB_info_type',\n",
    "                        'peeringDB_policy_general'\n",
    "                        'ASDB_C1L1']\n",
    "\n",
    "list_feat = LIST_FEATURES_NO_CATEG + LIST_FEATURES_CATEG\n",
    "\n",
    "features_file = 'node_degrees' #'' # 'node_degrees' # las features que se le agregaran seran \n",
    "type = \"DiGraph\" # MultiDiGraph\n",
    "relationships_file\n",
    "max_paths = 100000\n",
    "graph_case1 = create_files(type, \n",
    "            dataset_graph_path,\n",
    "            file = rib_path, \n",
    "            features_file = features_file, \n",
    "            from_caida=False, \n",
    "            remove_degree=3,\n",
    "            debug=True,\n",
    "            max_paths = max_paths)\n",
    " \n",
    "print('[NX Graph]',graph_case1.nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso 2\n",
    "* Creacion del grafo a partir de CAIDA AS Relationships (AS Rank) \n",
    "* Se les da atributos a los edges correspondientes al tipo de relacion que comparten\n",
    "* Es solo de prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaRPETA CREADA]:  /home/valentina/Desktop/GIT/TrabajoTesis/data/dgl_graph/\n",
      "[NX Graph]:  DiGraph with 74140 nodes and 379420 edges\n",
      "[TOPOLOGIA CREADA]\n",
      "[todos attr]\n",
      "[bbbbbbbbbbbbbbbbbbbbbb]\n",
      "[NX Graph]:  DiGraph with 74140 nodes and 379420 edges\n",
      "[FEATURES CREADOS]\n",
      "[META CREADO]\n",
      "[NX Graph]:  DiGraph with 46714 nodes and 351994 edges\n",
      "[NX Graph]:  DiGraph with 46237 nodes and 351517 edges\n",
      "[NX Graph]:  DiGraph with 46229 nodes and 351509 edges\n",
      "[NX Graph] DiGraph with 46229 nodes and 351509 edges\n"
     ]
    }
   ],
   "source": [
    "# Definimos las listas de features\n",
    "LIST_FEATURES_NO_CATEG = ['ASN', 'AS_rank_numberAsns', 'AS_rank_numberPrefixes', 'AS_rank_numberAddresses', 'AS_rank_total', 'AS_rank_customer',\n",
    "                 'AS_rank_peer', 'AS_rank_provider', 'peeringDB_ix_count', 'peeringDB_fac_count', 'AS_hegemony', 'cti_top', 'cti_origin']\n",
    "\n",
    "LIST_FEATURES_CATEG = ['AS_rank_continent',\n",
    "                        'peeringDB_info_ratio',\n",
    "                        'peeringDB_info_scope',\n",
    "                        'peeringDB_info_type',\n",
    "                        'peeringDB_policy_general'\n",
    "                        'ASDB_C1L1']\n",
    "\n",
    "list_feat = LIST_FEATURES_NO_CATEG + LIST_FEATURES_CATEG\n",
    "\n",
    "features_file =  'data/node_features.csv'  #'node_degrees' #'' # 'node_degrees' # las features que se le agregaran seran \n",
    "type = \"DiGraph\" # MultiDiGraph\n",
    "\n",
    "max_paths = 100000\n",
    "graph_case1 = create_files(type, \n",
    "             dataset_graph_path,\n",
    "             relationships_file, \n",
    "             features_file, \n",
    "             from_caida=True, \n",
    "             remove_degree=3,\n",
    "             debug=True,\n",
    "max_paths = None)\n",
    " \n",
    "print('[NX Graph]',graph_case1.nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode-Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.dataloading import negative_sampler\n",
    "import dgl.function as fn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dgl.nn import SAGEConv, GraphConv, GATConv, GINConv, GatedGCNConv, GatedGraphConv\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn import SAGEConv, GraphConv, GATConv, GINConv, GatedGCNConv, GatedGraphConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(pos_score,neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    )\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Prediction\n",
    "Encoder : GNN\n",
    "Decoder : DotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=36173, num_edges=52756,\n",
      "      ndata_schemes={'feat': Scheme(shape=(2,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.float32)})\n",
      "[ATTR SHAPE]:  torch.Size([36173, 2])\n",
      "Generando 52756 aristas negativas...\n",
      "Aristas negativas generadas: 52755\n"
     ]
    }
   ],
   "source": [
    "gnn = GNN(debug=True)\n",
    "gnn.load_dataset(dataset_graph_path, force_reload=True)\n",
    "\n",
    "# FIXME: Cambiar, por mientras se estan agregando feat aleatorias o 1s a los nodos\n",
    "# gnn.dgl_graph.ndata['feat'] = torch.ones(gnn.dgl_graph.num_nodes(), 128)            # features de tamano 128\n",
    "\n",
    "print('[ATTR SHAPE]: ',gnn.dgl_graph.ndata['feat'].shape)\n",
    "\n",
    "gnn.split_graph(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 2.2169077396392822\n",
      "In epoch 5, loss: 0.8700197339057922\n",
      "In epoch 10, loss: 0.5841309428215027\n",
      "In epoch 15, loss: 0.5133039951324463\n",
      "In epoch 20, loss: 0.48531290888786316\n",
      "In epoch 25, loss: 0.468147873878479\n",
      "In epoch 30, loss: 0.45938563346862793\n",
      "In epoch 35, loss: 0.44982054829597473\n",
      "In epoch 40, loss: 0.4390973448753357\n",
      "In epoch 45, loss: 0.429028183221817\n",
      "In epoch 50, loss: 0.42089155316352844\n",
      "In epoch 55, loss: 0.41420429944992065\n",
      "In epoch 60, loss: 0.409568727016449\n",
      "In epoch 65, loss: 0.4064272940158844\n",
      "In epoch 70, loss: 0.40386268496513367\n",
      "In epoch 75, loss: 0.40169769525527954\n",
      "In epoch 80, loss: 0.39999765157699585\n",
      "In epoch 85, loss: 0.39872145652770996\n",
      "In epoch 90, loss: 0.39759910106658936\n",
      "In epoch 95, loss: 0.39672160148620605\n",
      "AUC 0.956555513128636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GraphSAGE(gnn.train_g.ndata[\"feat\"].shape[1], 16,16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    model.train()\n",
    "    h = model.encode(gnn.train_g, gnn.train_g.ndata[\"feat\"])\n",
    "\n",
    "    pos_score = model.decodeDotProduct(gnn.train_pos_g, h)\n",
    "    neg_score = model.decodeDotProduct(gnn.train_neg_g, h)\n",
    "\n",
    "\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_score = model.decodeDotProduct(gnn.test_pos_g, h)\n",
    "    neg_score = model.decodeDotProduct(gnn.test_neg_g, h)\n",
    "    print(\"AUC\", compute_auc(pos_score, neg_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Prediction\n",
    "Encoder : GNN\n",
    "Decoder : MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=74140, num_edges=351509,\n",
      "      ndata_schemes={'feat': Scheme(shape=(72,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.int64)})\n",
      "[ATTR SHAPE]:  torch.Size([74140, 72])\n",
      "Generando 351509 aristas negativas...\n",
      "Aristas negativas generadas: 351508\n"
     ]
    }
   ],
   "source": [
    "gnn = GNN(debug=True)\n",
    "gnn.load_dataset(dataset_graph_path, force_reload=True)\n",
    "\n",
    "# FIXME: Cambiar, por mientras se estan agregando feat aleatorias o 1s a los nodos\n",
    "# gnn.dgl_graph.ndata['feat'] = torch.ones(gnn.dgl_graph.num_nodes(), 128)            # features de tamano 128\n",
    "\n",
    "print('[ATTR SHAPE]: ',gnn.dgl_graph.ndata['feat'].shape)\n",
    "\n",
    "gnn.split_graph(train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.7252177000045776\n",
      "In epoch 5, loss: 0.6580637097358704\n",
      "In epoch 10, loss: 0.5534951090812683\n",
      "In epoch 15, loss: 0.384759783744812\n",
      "In epoch 20, loss: 0.244273841381073\n",
      "In epoch 25, loss: 0.1576494425535202\n",
      "In epoch 30, loss: 0.12169022113084793\n",
      "In epoch 35, loss: 0.10285701602697372\n",
      "In epoch 40, loss: 0.09458532184362411\n",
      "In epoch 45, loss: 0.08908131718635559\n",
      "In epoch 50, loss: 0.08555731177330017\n",
      "In epoch 55, loss: 0.08315407484769821\n",
      "In epoch 60, loss: 0.08149542659521103\n",
      "In epoch 65, loss: 0.08005796372890472\n",
      "In epoch 70, loss: 0.0789530873298645\n",
      "In epoch 75, loss: 0.07803794741630554\n",
      "In epoch 80, loss: 0.07713112235069275\n",
      "In epoch 85, loss: 0.07627647370100021\n",
      "In epoch 90, loss: 0.07546034455299377\n",
      "In epoch 95, loss: 0.0746576339006424\n",
      "AUC 0.9855221090672165\n"
     ]
    }
   ],
   "source": [
    "model = GraphSAGE(gnn.train_g.ndata[\"feat\"].shape[1], 16,16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    model.train()\n",
    "    h = model.encode(gnn.train_g, gnn.train_g.ndata[\"feat\"])\n",
    "\n",
    "    pos_score = model.decodeMLP(gnn.train_pos_g, h)\n",
    "    neg_score = model.decodeMLP(gnn.train_neg_g, h)\n",
    "\n",
    "\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_score = model.decodeMLP(gnn.test_pos_g, h)\n",
    "    neg_score = model.decodeMLP(gnn.test_neg_g, h)\n",
    "    print(\"AUC\", compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.fc_neigh.weight torch.Size([16, 72])\n",
      "conv1.fc_self.weight torch.Size([16, 72])\n",
      "conv1.fc_self.bias torch.Size([16])\n",
      "conv2.fc_neigh.weight torch.Size([16, 16])\n",
      "conv2.fc_self.weight torch.Size([16, 16])\n",
      "conv2.fc_self.bias torch.Size([16])\n",
      "decoder.W1.weight torch.Size([16, 32])\n",
      "decoder.W1.bias torch.Size([16])\n",
      "decoder.W2.weight torch.Size([1, 16])\n",
      "decoder.W2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Se incluyen los parametros tanto de MLP como de la GNN\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar Modelo y Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guaradar el modelo\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Guardar los embeddings\n",
    "torch.save(h, \"embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings guardados en 'node_embeddings.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.960633</td>\n",
       "      <td>-0.530889</td>\n",
       "      <td>-0.063146</td>\n",
       "      <td>3.337903</td>\n",
       "      <td>-6.670915</td>\n",
       "      <td>2.733388</td>\n",
       "      <td>5.843852</td>\n",
       "      <td>3.043777</td>\n",
       "      <td>-2.202881</td>\n",
       "      <td>-10.431517</td>\n",
       "      <td>-1.514594</td>\n",
       "      <td>-6.829981</td>\n",
       "      <td>-8.230904</td>\n",
       "      <td>5.781105</td>\n",
       "      <td>2.572767</td>\n",
       "      <td>0.408480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.474360</td>\n",
       "      <td>-0.500746</td>\n",
       "      <td>0.598364</td>\n",
       "      <td>3.052859</td>\n",
       "      <td>-6.689769</td>\n",
       "      <td>1.265362</td>\n",
       "      <td>5.260845</td>\n",
       "      <td>2.683862</td>\n",
       "      <td>-2.719274</td>\n",
       "      <td>-9.995238</td>\n",
       "      <td>-0.968039</td>\n",
       "      <td>-6.013503</td>\n",
       "      <td>-7.597056</td>\n",
       "      <td>5.277479</td>\n",
       "      <td>2.604416</td>\n",
       "      <td>-0.071746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.691902</td>\n",
       "      <td>-0.606663</td>\n",
       "      <td>-1.135238</td>\n",
       "      <td>3.490635</td>\n",
       "      <td>-7.177549</td>\n",
       "      <td>3.193054</td>\n",
       "      <td>6.032834</td>\n",
       "      <td>3.134146</td>\n",
       "      <td>-2.324368</td>\n",
       "      <td>-10.792953</td>\n",
       "      <td>-2.193590</td>\n",
       "      <td>-8.526873</td>\n",
       "      <td>-9.285971</td>\n",
       "      <td>6.201936</td>\n",
       "      <td>2.646856</td>\n",
       "      <td>0.946869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.065714</td>\n",
       "      <td>-0.306806</td>\n",
       "      <td>-1.747749</td>\n",
       "      <td>3.754204</td>\n",
       "      <td>-6.717747</td>\n",
       "      <td>3.283868</td>\n",
       "      <td>5.309824</td>\n",
       "      <td>3.119607</td>\n",
       "      <td>-0.934983</td>\n",
       "      <td>-10.517172</td>\n",
       "      <td>-2.546461</td>\n",
       "      <td>-7.387640</td>\n",
       "      <td>-7.450150</td>\n",
       "      <td>4.827490</td>\n",
       "      <td>1.935872</td>\n",
       "      <td>1.801739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.342475</td>\n",
       "      <td>0.460370</td>\n",
       "      <td>-1.314342</td>\n",
       "      <td>2.776710</td>\n",
       "      <td>-5.806755</td>\n",
       "      <td>3.219721</td>\n",
       "      <td>7.075166</td>\n",
       "      <td>2.901008</td>\n",
       "      <td>-1.985440</td>\n",
       "      <td>-10.643597</td>\n",
       "      <td>-2.596992</td>\n",
       "      <td>-6.497208</td>\n",
       "      <td>-7.706456</td>\n",
       "      <td>4.182283</td>\n",
       "      <td>3.250054</td>\n",
       "      <td>-0.026504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74135</th>\n",
       "      <td>-0.644776</td>\n",
       "      <td>-0.340468</td>\n",
       "      <td>-0.916077</td>\n",
       "      <td>-0.612864</td>\n",
       "      <td>0.486238</td>\n",
       "      <td>0.940660</td>\n",
       "      <td>-1.089343</td>\n",
       "      <td>-0.368585</td>\n",
       "      <td>0.568008</td>\n",
       "      <td>0.174415</td>\n",
       "      <td>-0.495718</td>\n",
       "      <td>-1.449336</td>\n",
       "      <td>-0.532172</td>\n",
       "      <td>1.557938</td>\n",
       "      <td>-0.274276</td>\n",
       "      <td>1.011504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74136</th>\n",
       "      <td>4.297664</td>\n",
       "      <td>1.034229</td>\n",
       "      <td>-3.909623</td>\n",
       "      <td>3.547935</td>\n",
       "      <td>-7.930598</td>\n",
       "      <td>5.262478</td>\n",
       "      <td>6.749265</td>\n",
       "      <td>2.603194</td>\n",
       "      <td>0.681387</td>\n",
       "      <td>-11.181349</td>\n",
       "      <td>-4.115701</td>\n",
       "      <td>-10.065962</td>\n",
       "      <td>-7.557868</td>\n",
       "      <td>6.250222</td>\n",
       "      <td>2.218752</td>\n",
       "      <td>2.107581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74137</th>\n",
       "      <td>4.699925</td>\n",
       "      <td>1.784898</td>\n",
       "      <td>-6.673481</td>\n",
       "      <td>2.243041</td>\n",
       "      <td>-9.939831</td>\n",
       "      <td>5.294265</td>\n",
       "      <td>7.239003</td>\n",
       "      <td>2.592410</td>\n",
       "      <td>1.117161</td>\n",
       "      <td>-12.087595</td>\n",
       "      <td>-4.150962</td>\n",
       "      <td>-10.454863</td>\n",
       "      <td>-7.831730</td>\n",
       "      <td>6.572432</td>\n",
       "      <td>1.784056</td>\n",
       "      <td>2.385424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74138</th>\n",
       "      <td>5.091138</td>\n",
       "      <td>0.435841</td>\n",
       "      <td>-3.018356</td>\n",
       "      <td>4.150651</td>\n",
       "      <td>-8.626420</td>\n",
       "      <td>2.210055</td>\n",
       "      <td>8.485260</td>\n",
       "      <td>3.748398</td>\n",
       "      <td>-2.397845</td>\n",
       "      <td>-11.802258</td>\n",
       "      <td>-3.035294</td>\n",
       "      <td>-7.054545</td>\n",
       "      <td>-7.301067</td>\n",
       "      <td>5.770293</td>\n",
       "      <td>1.883959</td>\n",
       "      <td>-0.711655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74139</th>\n",
       "      <td>-0.582012</td>\n",
       "      <td>-0.704478</td>\n",
       "      <td>-0.814912</td>\n",
       "      <td>-0.415842</td>\n",
       "      <td>0.455447</td>\n",
       "      <td>0.818103</td>\n",
       "      <td>-1.353062</td>\n",
       "      <td>-0.466690</td>\n",
       "      <td>0.297422</td>\n",
       "      <td>0.248338</td>\n",
       "      <td>-0.360341</td>\n",
       "      <td>-1.773293</td>\n",
       "      <td>-1.002193</td>\n",
       "      <td>2.324361</td>\n",
       "      <td>-0.567931</td>\n",
       "      <td>1.278515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74140 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6   \\\n",
       "node_id                                                                         \n",
       "0        2.960633 -0.530889 -0.063146  3.337903 -6.670915  2.733388  5.843852   \n",
       "1        3.474360 -0.500746  0.598364  3.052859 -6.689769  1.265362  5.260845   \n",
       "2        3.691902 -0.606663 -1.135238  3.490635 -7.177549  3.193054  6.032834   \n",
       "3        2.065714 -0.306806 -1.747749  3.754204 -6.717747  3.283868  5.309824   \n",
       "4        2.342475  0.460370 -1.314342  2.776710 -5.806755  3.219721  7.075166   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "74135   -0.644776 -0.340468 -0.916077 -0.612864  0.486238  0.940660 -1.089343   \n",
       "74136    4.297664  1.034229 -3.909623  3.547935 -7.930598  5.262478  6.749265   \n",
       "74137    4.699925  1.784898 -6.673481  2.243041 -9.939831  5.294265  7.239003   \n",
       "74138    5.091138  0.435841 -3.018356  4.150651 -8.626420  2.210055  8.485260   \n",
       "74139   -0.582012 -0.704478 -0.814912 -0.415842  0.455447  0.818103 -1.353062   \n",
       "\n",
       "               7         8          9         10         11        12  \\\n",
       "node_id                                                                 \n",
       "0        3.043777 -2.202881 -10.431517 -1.514594  -6.829981 -8.230904   \n",
       "1        2.683862 -2.719274  -9.995238 -0.968039  -6.013503 -7.597056   \n",
       "2        3.134146 -2.324368 -10.792953 -2.193590  -8.526873 -9.285971   \n",
       "3        3.119607 -0.934983 -10.517172 -2.546461  -7.387640 -7.450150   \n",
       "4        2.901008 -1.985440 -10.643597 -2.596992  -6.497208 -7.706456   \n",
       "...           ...       ...        ...       ...        ...       ...   \n",
       "74135   -0.368585  0.568008   0.174415 -0.495718  -1.449336 -0.532172   \n",
       "74136    2.603194  0.681387 -11.181349 -4.115701 -10.065962 -7.557868   \n",
       "74137    2.592410  1.117161 -12.087595 -4.150962 -10.454863 -7.831730   \n",
       "74138    3.748398 -2.397845 -11.802258 -3.035294  -7.054545 -7.301067   \n",
       "74139   -0.466690  0.297422   0.248338 -0.360341  -1.773293 -1.002193   \n",
       "\n",
       "               13        14        15  \n",
       "node_id                                \n",
       "0        5.781105  2.572767  0.408480  \n",
       "1        5.277479  2.604416 -0.071746  \n",
       "2        6.201936  2.646856  0.946869  \n",
       "3        4.827490  1.935872  1.801739  \n",
       "4        4.182283  3.250054 -0.026504  \n",
       "...           ...       ...       ...  \n",
       "74135    1.557938 -0.274276  1.011504  \n",
       "74136    6.250222  2.218752  2.107581  \n",
       "74137    6.572432  1.784056  2.385424  \n",
       "74138    5.770293  1.883959 -0.711655  \n",
       "74139    2.324361 -0.567931  1.278515  \n",
       "\n",
       "[74140 rows x 16 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Guaradar el modelo\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Guardar los embeddings de los nodos después del entrenamiento\n",
    "with torch.no_grad():\n",
    "    # Calcular los embeddings finales\n",
    "    # final_embeddings = model.encode(train_g, train_g.ndata[\"feat\"]).detach().cpu().numpy()\n",
    "    final_embeddings = h.detach().cpu().numpy()\n",
    "\n",
    "    # Crear un DataFrame para guardar los embeddings\n",
    "    # Obtener ids de los nodos (ASN)\n",
    "    # node_ids = train_g.nodes().numpy()\n",
    "    node_ids = gnn.dgl_graph.nodes().numpy()\n",
    "\n",
    "    emb_df = pd.DataFrame(final_embeddings, index=node_ids)\n",
    "    emb_df.index.name = \"node_id\"\n",
    "\n",
    "    # Guardar en un archivo CSV\n",
    "    emb_df.to_csv(\"node_embeddings.csv\")\n",
    "    np.save(\"node_embeddings.npy\", final_embeddings)\n",
    "    print(\"Embeddings guardados en 'node_embeddings.csv'\")\n",
    "\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAPE] (74140, 16)\n",
      "[EMBEDDING] [  2.9606333   -0.5308888   -0.06314588   3.3379025   -6.6709146\n",
      "   2.733388     5.8438516    3.0437765   -2.2028809  -10.431517\n",
      "  -1.5145942   -6.829981    -8.230904     5.781105     2.5727673\n",
      "   0.4084804 ]\n"
     ]
    }
   ],
   "source": [
    "# Cargar embeddings\n",
    "embeddings = np.load(\"node_embeddings.npy\")\n",
    "print(\"[SHAPE]\",embeddings.shape)\n",
    "print(\"[EMBEDDING]\",embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosas a tener consideraciom:\n",
    "- GNN al crear un grafo, crea tdoos los nodos que se encuentran en el rango entregado. De esta forma pueden quedar nodos aislados, sin embargo ento causa que no se puedan ir 'actualizando' pero al agregar self_loop se actualizan con si mismos y supondremos que con ello algun patron para este tipo de nodos.\n",
    "- La entrega de embeddings finales esta en orden el ASN de valor inferios hasta el ASN de valor maximo.\n",
    "- Lo guardamso en un .csv dondde se asocia 'node_id' con su embeddings y donde node_id es el ASN del Sistema Autonomo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1088, 0.3678, 0.4565,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1593, 0.4536, 0.5653,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3673, 0.6768,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0813, 0.2908,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1853, 0.3537, 0.5493,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0513, 0.2586,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.train_g.ndata['feat'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "Primero:\n",
    "* Crear grafo GNN a partir de AS rank y los atributos de ese otro dataset de antes.\n",
    "* Con esa informacion entrenar los embedings y luego pasarle esos embedigs a a la clasificacion\n",
    "\n",
    "Segundo:\n",
    "* Probra crear grafo con archivos oix \n",
    "* Luego entrenar para clasificacion\n",
    "\n",
    "Tercero:\n",
    "* Copiar la Red Neuronal que ocupa BGP2Vec\n",
    "* Probar con los embeddings anteriores probar la clasificacion. \n",
    "Si hay buenos resultados significa que nuestra MLP es muy basica/simple/poco compleja para atrapar las relaciones, identificar patrones. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
