{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Embeddings con GNN\n",
    "Aca crearemos embeddings paa un grafo de Internet, es decir represenaciones de los SA a partir de la topologia y atributos de los SA.\n",
    "\n",
    "Para esto tomamos 3 enfoques:\n",
    "\n",
    "\n",
    "*   [Caso 1] Reconstruction Approach autoencoder: A traves de la prediccion de aristas construimos los embeddings de los nodos.\n",
    "*   [Caso 1.1] Reconstruction Approach autoencoder: en ves de ocupar las bgp routes recolectadas de los ribs ocupamos las relaciones de AS rank y le damso attr a las aristas (es solo para probar que lo estemos ahciendo bien y las cosas tienen sentido)\n",
    "*   [Caso 2]: Reconstruction Approach attribute Masking:\n",
    "*   Caso 3: Task Generation Pre-calculated descriptor\n",
    "\n",
    "Para cada uno crearemos un ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from modules.gnn import GNN\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from modules.graph import Graph, create_files\n",
    "from modules.gnn import GNN\n",
    "from modules.gnn_models import GCN, GraphSAGE\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las rutas de los archivos\n",
    "base_path = os.getcwd() + \"/data/\"\n",
    "relationships_file = base_path + \"CAIDA_AS_Relationships/Serial_1/20220701.as-rel.txt.bz2\"\n",
    "features_file = base_path + \"/node_features.csv\"\n",
    "rib_path = base_path + 'sanitized_rib.txt'\n",
    "dataset_graph_path = base_path + 'dgl_graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion Grafo \n",
    "Creamos un grafo nx y dgl, ademas de los archivos edges.csv y nodes.csv a partir de archivos ribs previamente creados o de archivo CAIDA AS Relationships.\n",
    "\n",
    "Crear esos archivos una unica vez con create_graph() una ves ya creados los archivos edges.csv y nodes.csv puedo ocupar directamente la funcion \n",
    "\n",
    "Se le puede indicar el maximo de bgp paths que se quiere (hehco para cuando se leen ribs no de caida) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASO 1: RIBs\n",
    "* Creacion de grafo a partir de paths recolectados de las RIBs por BGPStream\n",
    "* Por ahora le asignamos a todos los nodos embeddings iniciales de de dimension 32 parte con puros 1s todos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaRPETA CREADA]:  /home/valentina/Desktop/GIT/TrabajoTesis/data/dgl_graph/\n",
      "[Creando topologia]\n",
      "[ARCHIVO EDGES.CSV CREADO]\n",
      "[NX Graph]:  MultiDiGraph with 36580 nodes and 325860 edges\n",
      "[Agregando attr]\n",
      "[NX Graph]:  MultiDiGraph with 36580 nodes and 325860 edges\n",
      "nodes.csv edges.csv\n",
      "[FEATURES CREADOS]\n",
      "[META CREADO]\n",
      "[NX Graph]:  MultiDiGraph with 26584 nodes and 315864 edges\n",
      "[NX Graph]:  MultiDiGraph with 26495 nodes and 315775 edges\n",
      "[NX Graph]:  MultiDiGraph with 26494 nodes and 315774 edges\n",
      "[NX Graph] MultiDiGraph with 26494 nodes and 315774 edges\n"
     ]
    }
   ],
   "source": [
    "# Definimos las listas de features\n",
    "\n",
    "# features_file = 'node_degrees' #'' # 'node_degrees' # las features que se le agregaran seran \n",
    "# features_file = base_path + \"/node_features.csv\"\n",
    "features_file = base_path + \"/node_features_mio.csv\"\n",
    "type = 'MultiDiGraph'#\"DiGraph\" # MultiDiGraph\n",
    "relationships_file\n",
    "MAX_PATHS = 100000\n",
    "graph_case1 = create_files(type, \n",
    "            dataset_graph_path,\n",
    "            file = rib_path, \n",
    "            features_file = features_file, \n",
    "            from_caida=False, \n",
    "            remove_degree=3,\n",
    "            debug=True,\n",
    "            max_paths = MAX_PATHS)\n",
    " \n",
    "print('[NX Graph]',graph_case1.nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASO 2: CAIDA Relationships\n",
    "* Creacion del grafo a partir de CAIDA AS Relationships (AS Rank) \n",
    "* Se les da atributos a los edges correspondientes al tipo de relacion que comparten\n",
    "* Es solo de prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CaRPETA CREADA]:  /home/valentina/Desktop/GIT/TrabajoTesis/data/dgl_graph/\n",
      "[NX Graph]:  DiGraph with 74140 nodes and 379420 edges\n",
      "[TOPOLOGIA CREADA]\n",
      "[todos attr]\n",
      "[bbbbbbbbbbbbbbbbbbbbbb]\n",
      "[NX Graph]:  DiGraph with 74140 nodes and 379420 edges\n",
      "nodes.csv edges.csv\n",
      "[FEATURES CREADOS]\n",
      "[META CREADO]\n",
      "[NX Graph]:  DiGraph with 46714 nodes and 351994 edges\n",
      "[NX Graph]:  DiGraph with 46237 nodes and 351517 edges\n",
      "[NX Graph]:  DiGraph with 46229 nodes and 351509 edges\n",
      "[NX Graph] DiGraph with 46229 nodes and 351509 edges\n"
     ]
    }
   ],
   "source": [
    "# Definimos las listas de features\n",
    "\n",
    "features_file =  'data/node_features.csv'  #'node_degrees' #'' # 'node_degrees' # las features que se le agregaran seran \n",
    "type = \"DiGraph\" # MultiDiGraph\n",
    "\n",
    "max_paths = 100000\n",
    "graph_case1 = create_files(type, \n",
    "             dataset_graph_path,\n",
    "             relationships_file, \n",
    "             features_file, \n",
    "             from_caida=True, \n",
    "             remove_degree=3,\n",
    "             debug=True,\n",
    "max_paths = None)\n",
    " \n",
    "print('[NX Graph]',graph_case1.nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction: Encode-Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.dataloading import negative_sampler\n",
    "import dgl.function as fn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dgl.nn import SAGEConv, GraphConv, GATConv, GINConv, GatedGCNConv, GatedGraphConv\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn import SAGEConv, GraphConv, GATConv, GINConv, GatedGCNConv, GatedGraphConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(pos_score,neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    )\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASO 1: \n",
    "* Encoder : GNN\n",
    "* Decoder : DotProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=36580, num_edges=315774,\n",
      "      ndata_schemes={'feat': Scheme(shape=(69,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.float32)})\n",
      "[ATTR SHAPE]:  torch.Size([36580, 69])\n",
      "Generando 315774 aristas negativas...\n",
      "Aristas negativas generadas: 315773\n"
     ]
    }
   ],
   "source": [
    "gnn = GNN(debug=True)\n",
    "gnn.load_dataset(dataset_graph_path, force_reload=True)\n",
    "\n",
    "print('[ATTR SHAPE]: ',gnn.dgl_graph.ndata['feat'].shape)\n",
    "\n",
    "gnn.split_graph_edges(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 2.3666417598724365\n",
      "In epoch 5, loss: 0.6438419818878174\n",
      "In epoch 10, loss: 0.5738990306854248\n",
      "In epoch 15, loss: 0.5256621241569519\n",
      "In epoch 20, loss: 0.5031626224517822\n",
      "In epoch 25, loss: 0.4877111613750458\n",
      "In epoch 30, loss: 0.47665834426879883\n",
      "In epoch 35, loss: 0.46760863065719604\n",
      "In epoch 40, loss: 0.46020612120628357\n",
      "In epoch 45, loss: 0.45447930693626404\n",
      "In epoch 50, loss: 0.4497029781341553\n",
      "In epoch 55, loss: 0.4458879828453064\n",
      "In epoch 60, loss: 0.4427325427532196\n",
      "In epoch 65, loss: 0.44007453322410583\n",
      "In epoch 70, loss: 0.43783459067344666\n",
      "In epoch 75, loss: 0.43594813346862793\n",
      "In epoch 80, loss: 0.434280127286911\n",
      "In epoch 85, loss: 0.4327625334262848\n",
      "In epoch 90, loss: 0.4313984811306\n",
      "In epoch 95, loss: 0.4301387667655945\n",
      "AUC 0.9432483659934531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GraphSAGE(gnn.train_g.ndata[\"feat\"].shape[1], 16,16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    model.train()\n",
    "    h = model.encode(gnn.train_g, gnn.train_g.ndata[\"feat\"])\n",
    "\n",
    "    pos_score = model.decodeDotProduct(gnn.train_pos_g, h)\n",
    "    neg_score = model.decodeDotProduct(gnn.train_neg_g, h)\n",
    "\n",
    "\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_score = model.decodeDotProduct(gnn.test_pos_g, h)\n",
    "    neg_score = model.decodeDotProduct(gnn.test_neg_g, h)\n",
    "    print(\"AUC\", compute_auc(pos_score, neg_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guaradar el modelo\n",
    "torch.save(model.state_dict(), 'data/model_emb.pth')\n",
    "\n",
    "# Guardar los embeddings\n",
    "torch.save(h, \"data/embeddings_ribs_DP.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASO 2: \n",
    "* Encoder : GNN\n",
    "* Decoder : MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=36580, num_edges=315774,\n",
      "      ndata_schemes={'feat': Scheme(shape=(69,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.float32)})\n",
      "[ATTR SHAPE]:  torch.Size([36580, 69])\n",
      "Generando 315774 aristas negativas...\n",
      "Aristas negativas generadas: 315773\n"
     ]
    }
   ],
   "source": [
    "gnn = GNN(debug=True)\n",
    "gnn.load_dataset(dataset_graph_path, force_reload=True)\n",
    "\n",
    "# FIXME: Cambiar, por mientras se estan agregando feat aleatorias o 1s a los nodos\n",
    "# gnn.dgl_graph.ndata['feat'] = torch.ones(gnn.dgl_graph.num_nodes(), 128)            # features de tamano 128\n",
    "\n",
    "print('[ATTR SHAPE]: ',gnn.dgl_graph.ndata['feat'].shape)\n",
    "\n",
    "gnn.split_graph_edges(train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.6792717576026917\n",
      "In epoch 5, loss: 0.42843642830848694\n",
      "In epoch 10, loss: 0.2641475796699524\n",
      "In epoch 15, loss: 0.20924416184425354\n",
      "In epoch 20, loss: 0.17611178755760193\n",
      "In epoch 25, loss: 0.15358121693134308\n",
      "In epoch 30, loss: 0.1409989595413208\n",
      "In epoch 35, loss: 0.1279410719871521\n",
      "In epoch 40, loss: 0.1201382726430893\n",
      "In epoch 45, loss: 0.1134682297706604\n",
      "AUC 0.9879080912494592\n"
     ]
    }
   ],
   "source": [
    "model = GraphSAGE(gnn.train_g.ndata[\"feat\"].shape[1], 16,16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "\n",
    "all_logits = []\n",
    "for e in range(50):\n",
    "    # forward\n",
    "    model.train()\n",
    "    h = model.encode(gnn.train_g, gnn.train_g.ndata[\"feat\"])\n",
    "\n",
    "    pos_score = model.decodeMLP(gnn.train_pos_g, h)\n",
    "    neg_score = model.decodeMLP(gnn.train_neg_g, h)\n",
    "\n",
    "\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_score = model.decodeMLP(gnn.test_pos_g, h)\n",
    "    neg_score = model.decodeMLP(gnn.test_neg_g, h)\n",
    "    print(\"AUC\", compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.fc_neigh.weight torch.Size([16, 69])\n",
      "conv1.fc_self.weight torch.Size([16, 69])\n",
      "conv1.fc_self.bias torch.Size([16])\n",
      "conv2.fc_neigh.weight torch.Size([16, 16])\n",
      "conv2.fc_self.weight torch.Size([16, 16])\n",
      "conv2.fc_self.bias torch.Size([16])\n",
      "decoder.W1.weight torch.Size([16, 32])\n",
      "decoder.W1.bias torch.Size([16])\n",
      "decoder.W2.weight torch.Size([1, 16])\n",
      "decoder.W2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Se incluyen los parametros tanto de MLP como de la GNN\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar Modelo y Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guaradar el modelo\n",
    "torch.save(model.state_dict(), 'data/model_emb.pth')\n",
    "\n",
    "# Guardar los embeddings\n",
    "torch.save(h, \"data/embeddings_ribs_MLP.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosas a tener consideraciom:\n",
    "- GNN al crear un grafo, crea tdoos los nodos que se encuentran en el rango entregado. De esta forma pueden quedar nodos aislados, sin embargo ento causa que no se puedan ir 'actualizando' pero al agregar self_loop se actualizan con si mismos y supondremos que con ello algun patron para este tipo de nodos.\n",
    "- La entrega de embeddings finales esta en orden el ASN de valor inferios hasta el ASN de valor maximo.\n",
    "- Lo guardamso en un .csv dondde se asocia 'node_id' con su embeddings y donde node_id es el ASN del Sistema Autonomo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia valor PAgeRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "\n",
    "class GNNPredictLastFeat(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_feats, hidden_feats)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_feats, hidden_feats)\n",
    "        self.regressor = nn.Linear(hidden_feats, 1)  # solo predice 1 número por nodo\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = F.relu(self.conv1(g, features))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        out = self.regressor(h)  # predicción de un valor\n",
    "        return out.squeeze(-1)   # tamaño [n_nodos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=36580, num_edges=315774,\n",
      "      ndata_schemes={'feat': Scheme(shape=(69,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.float32)})\n"
     ]
    }
   ],
   "source": [
    "gnn = GNN(debug=True)\n",
    "gnn.load_dataset(dataset_graph_path, force_reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar valor a inferir\n",
    "feats = gnn.dgl_graph.ndata['feat']\n",
    "targets = feats[:, -1]\n",
    "new_feats = feats[:, :-1]\n",
    "\n",
    "gnn.dgl_graph.ndata['feat'] = new_feats\n",
    "gnn.dgl_graph = dgl.add_self_loop(gnn.dgl_graph) # FIXME:\n",
    "\n",
    "gnn.split_graph_nodes(train_size=0.8)\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  ...,  True, False, False])\n",
      "tensor([ True, False, False,  ..., False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_mask = gnn.dgl_graph.train_mask\n",
    "test_mask = gnn.dgl_graph.test_mask\n",
    "\n",
    "print(train_mask)\n",
    "print(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0177\n",
      "Epoch 1: Loss = 0.0045\n",
      "Epoch 2: Loss = 0.0078\n",
      "Epoch 3: Loss = 0.0035\n",
      "Epoch 4: Loss = 0.0010\n",
      "Epoch 5: Loss = 0.0005\n",
      "Epoch 6: Loss = 0.0004\n",
      "Epoch 7: Loss = 0.0004\n",
      "Epoch 8: Loss = 0.0003\n",
      "Epoch 9: Loss = 0.0004\n",
      "Epoch 10: Loss = 0.0004\n",
      "Epoch 11: Loss = 0.0003\n",
      "Epoch 12: Loss = 0.0001\n",
      "Epoch 13: Loss = 0.0001\n",
      "Epoch 14: Loss = 0.0001\n",
      "Epoch 15: Loss = 0.0001\n",
      "Epoch 16: Loss = 0.0001\n",
      "Epoch 17: Loss = 0.0001\n",
      "Epoch 18: Loss = 0.0001\n",
      "Epoch 19: Loss = 0.0001\n",
      "Epoch 20: Loss = 0.0001\n",
      "Epoch 21: Loss = 0.0001\n",
      "Epoch 22: Loss = 0.0000\n",
      "Epoch 23: Loss = 0.0000\n",
      "Epoch 24: Loss = 0.0000\n",
      "Epoch 25: Loss = 0.0000\n",
      "Epoch 26: Loss = 0.0000\n",
      "Epoch 27: Loss = 0.0000\n",
      "Epoch 28: Loss = 0.0000\n",
      "Epoch 29: Loss = 0.0000\n",
      "Epoch 30: Loss = 0.0000\n",
      "Epoch 31: Loss = 0.0000\n",
      "Epoch 32: Loss = 0.0000\n",
      "Epoch 33: Loss = 0.0000\n",
      "Epoch 34: Loss = 0.0000\n",
      "Epoch 35: Loss = 0.0000\n",
      "Epoch 36: Loss = 0.0000\n",
      "Epoch 37: Loss = 0.0000\n",
      "Epoch 38: Loss = 0.0000\n",
      "Epoch 39: Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# -------------- Entrenamiento ------------------ #\n",
    "import torch.optim as optim\n",
    "\n",
    "# Creamos el modelo\n",
    "model = GNNPredictLastFeat(in_feats=gnn.dgl_graph.ndata['feat'].shape[1], hidden_feats=64)\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()  # Porque es regresión\n",
    "\n",
    "for epoch in range(40):\n",
    "    model.train()\n",
    "    logits = model(gnn.dgl_graph, gnn.dgl_graph.ndata['feat'])  # predictions.shape = [n_nodes]\n",
    "    loss = F.mse_loss(logits[train_mask], targets[train_mask])    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = model(gnn.dgl_graph, gnn.dgl_graph.ndata['feat'])\n",
    "    \n",
    "#     pred = (pred[test_mask] > 0.5).float()  # Convertir a 0 o 1\n",
    "#     acc = (pred == targets[test_mask]).float().mean()\n",
    "#     print(f\"Test Accuracy: {acc.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(gnn.dgl_graph, gnn.dgl_graph.ndata['feat'])\n",
    "\n",
    "mse = F.mse_loss(preds[test_mask], targets[test_mask]).item()\n",
    "print(f\"Test MSE: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36580, 68])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.dgl_graph.ndata['feat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guaradar el modelo\n",
    "torch.save(model.state_dict(), 'data/model_emb.pth')\n",
    "\n",
    "# Guardar los embeddings\n",
    "torch.save(h, \"data/embeddings_ribs_PageRank.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict otro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear Embeddings otros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import DeepWalk\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SparseAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=36580, num_edges=315774,\n",
      "      ndata_schemes={'feat': Scheme(shape=(74,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.float32)})\n",
      "[ATTR SHAPE]:  torch.Size([36580, 74])\n",
      "Generando 315774 aristas negativas...\n",
      "Aristas negativas generadas: 315773\n"
     ]
    }
   ],
   "source": [
    "gnn = GNN(debug=True)\n",
    "gnn.load_dataset(dataset_graph_path, force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = DeepWalk(gnn.dgl_graph, \n",
    "               emb_dim=32,        # tamaño del embedding\n",
    "               walk_length=6,     # número de saltos por walk\n",
    "            #    context_size=5,     # tamaño de la ventana de contexto\n",
    "            #    walks_per_node=10,  # número de walks por nodo\n",
    "            #    num_negative=5\n",
    "               )     # número de negativos por positivo\n",
    "\n",
    "emb = DeepWalk(gnn.dgl_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(torch.arange(gnn.dgl_graph.num_nodes()),\n",
    "                        batch_size=500,\n",
    "                        shuffle=True, \n",
    "                        collate_fn=emb.sample,)\n",
    "\n",
    "optimizer = SparseAdam(emb.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_walk in dataloader:\n",
    "        # Reemplazar -1 por un nodo válido (ej: nodo 0)\n",
    "        batch_walk[batch_walk == -1] = 0\n",
    "\n",
    "        loss = emb(batch_walk)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE torch.Size([36580, 128])\n",
      "EMBEDDINGS tensor([[-0.1586, -0.1637,  0.1585,  ...,  0.1625, -0.1555,  0.1694],\n",
      "        [-0.2038, -0.1939,  0.2022,  ...,  0.1929, -0.1953,  0.1814],\n",
      "        [-0.1803, -0.1698,  0.1698,  ...,  0.1676, -0.1702,  0.1691],\n",
      "        ...,\n",
      "        [-0.0862, -0.0907,  0.0921,  ...,  0.0857, -0.0850,  0.0945],\n",
      "        [-0.1030, -0.1055,  0.1089,  ...,  0.0963, -0.1074,  0.0997],\n",
      "        [-0.0967, -0.0872,  0.0959,  ...,  0.0849, -0.0953,  0.0819]])\n"
     ]
    }
   ],
   "source": [
    "h = emb.node_embed.weight.detach()\n",
    "print(\"SHAPE\",h.shape)\n",
    "print(\"EMBEDDINGS\",h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los embeddings\n",
    "torch.save(h, \"data/embeddings_deepWalk.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=36580, num_edges=315774,\n",
      "      ndata_schemes={'feat': Scheme(shape=(74,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.float32)})\n",
      "[ATTR SHAPE]:  torch.Size([36580, 74])\n",
      "Generando 315774 aristas negativas...\n",
      "Aristas negativas generadas: 315773\n"
     ]
    }
   ],
   "source": [
    "gnn = GNN(debug=True)\n",
    "gnn.load_dataset(dataset_graph_path, force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5807e-05],\n",
      "        [1.7451e-05],\n",
      "        [2.1136e-05],\n",
      "        ...,\n",
      "        [4.1371e-06],\n",
      "        [4.3104e-06],\n",
      "        [4.2975e-06]])\n"
     ]
    }
   ],
   "source": [
    "# ----------- 2. Run PageRank for Graph -------------- #\n",
    "N = gnn.dgl_graph.number_of_nodes()\n",
    "DAMP = 0.85\n",
    "K = 10\n",
    "\n",
    "\n",
    "def compute_pagerank(g):\n",
    "    g.ndata[\"pv\"] = torch.ones(N) / N\n",
    "    degrees = g.out_degrees(g.nodes()).type(torch.float32)\n",
    "    for k in range(K):\n",
    "        g.ndata[\"pv\"] = g.ndata[\"pv\"] / degrees\n",
    "        g.update_all(\n",
    "            message_func=fn.copy_u(u=\"pv\", out=\"m\"),\n",
    "            reduce_func=fn.sum(msg=\"m\", out=\"pv\"),\n",
    "        )\n",
    "        g.ndata[\"pv\"]  = (1 - DAMP) / N + DAMP * g.ndata[\"pv\"]\n",
    "    g.ndata[\"pv\"]  = g.ndata[\"pv\"].unsqueeze(1)\n",
    "    return g.ndata[\"pv\"]\n",
    "\n",
    "\n",
    "pv = compute_pagerank(gnn.dgl_graph)\n",
    "gnn.dgl_graph.ndata[\"h\"] = pv  # Inicializa características del nodo\n",
    "\n",
    "print(gnn.dgl_graph.ndata[\"h\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = gnn.dgl_graph.ndata[\"h\"]\n",
    "torch.save(h, \"data/embeddings_pageRank.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
