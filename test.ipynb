{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised training\n",
    "Aca crearemos embeddings paa un grafo de Internet, es decir represenaciones de los SA a partir de la topologia y atributos de los SA.\n",
    "\n",
    "Para esto tomamos 3 enfoques:\n",
    "\n",
    "\n",
    "*   Caso 1: Reconstruction Approach autoencoder\n",
    "*   Caso 2: Reconstruction Approach attribute Masking\n",
    "*   Caso 3: Task Generation Pre-calculated descriptor\n",
    "\n",
    "Para cada uno crearemos un ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentina/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importar librerias y RIBs\n",
    "\n",
    "from modules.gnn import GNN\n",
    "import scipy.sparse as sp\n",
    "\n",
    "rib_path = 'sanitized_rib.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafo NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de nodos: 42887\n",
      "Número de aristas: 367839\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Crear un grafo dirigido (BGP usa rutas direccionales)\n",
    "nx_graph = nx.DiGraph()\n",
    "\n",
    "# Leer el archivo y agregar aristas\n",
    "with open(rib_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        nodos = list(map(int, line.strip().split(\"|\")))  # Convertir a enteros\n",
    "        edges = zip(nodos, nodos[1:])  # Crear pares consecutivos\n",
    "        nx_graph.add_edges_from(edges)\n",
    "\n",
    "# Imprimir información del grafo\n",
    "print(f\"Número de nodos: {nx_graph.number_of_nodes()}\")\n",
    "print(f\"Número de aristas: {nx_graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafo DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=394239, num_edges=80557668,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "src, dst = [], []\n",
    "\n",
    "# Leer el archivo y extraer aristas\n",
    "with open(rib_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:  # Ignorar líneas vacías\n",
    "            continue\n",
    "        try:\n",
    "            nodos = list(map(int, line.split(\"|\")))\n",
    "            src.extend(nodos[:-1])  # Nodo de origen\n",
    "            dst.extend(nodos[1:])   # Nodo de destino\n",
    "        except ValueError as e:\n",
    "            print(f\"Error al procesar la línea: '{line}'. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Crear el grafo dirigido en DGL\n",
    "dgl_graph = dgl.graph((torch.tensor(src), torch.tensor(dst)))\n",
    "\n",
    "# Imprimir información del grafo\n",
    "print(dgl_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode-Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.dataloading import negative_sampler\n",
    "import dgl.function as fn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dgl.nn import SAGEConv, GraphConv, GATConv, GINConv, GatedGCNConv, GatedGraphConv\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo GCN\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_feats)\n",
    "        self.conv2 = GraphConv(hidden_feats, out_feats)\n",
    "    \n",
    "    def encode(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "    def decode(self, in_feat, edge_index):\n",
    "        return (in_feat[edge_index[0]] * in_feat[edge_index[1]]).sum(dim=-1)\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        return (z @ z.T) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_link_pred(dgl_graph, test_size=0.1):\n",
    "    u,v = dgl_graph.edges()\n",
    "\n",
    "    # IDs de lo edges\n",
    "    eids = np.arange(dgl_graph.num_edges()) \n",
    "    # Shuffle the edges\n",
    "    eids = np.random.permutation(eids)\n",
    "\n",
    "    # Tamaño de train y test\n",
    "    test_size = int(len(eids) * 0.1) \n",
    "    train_size = dgl_graph.num_edges() - test_size \n",
    "\n",
    "    # Selecciona los edges de test y train\n",
    "    test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "    train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "    # Matriz de adyacencia\n",
    "    adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "    # Matriz de adyacencia negativa\n",
    "    adj_neg = 1 - adj.todense() - np.eye(dgl_graph.num_nodes())\n",
    "\n",
    "    # IDs de lso edges negativos\n",
    "    neg_u, neg_v = np.where(adj_neg != 0) \n",
    "\n",
    "    # Selecciono nodos aleatorios (misma cantidad que el numero de edges positivos)\n",
    "    neg_eids = np.random.choice(len(neg_u), dgl_graph.num_edges())\n",
    "\n",
    "    test_neg_u, test_neg_v = (\n",
    "        neg_u[neg_eids[:test_size]],\n",
    "        neg_v[neg_eids[:test_size]],\n",
    "    )\n",
    "    train_neg_u, train_neg_v = (\n",
    "        neg_u[neg_eids[test_size:]],\n",
    "        neg_v[neg_eids[test_size:]],\n",
    "    )\n",
    "\n",
    "    train_g = dgl.remove_edges(dgl_graph, eids[:test_size])\n",
    "    \n",
    "    return  (train_pos_u, train_pos_v) , (test_pos_u, test_pos_v), (train_neg_u, train_neg_v), (test_neg_u, test_neg_v), train_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import dgl\n",
    "\n",
    "def split_data_link_pred(dgl_graph, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Divide los datos del grafo para la predicción de enlaces en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "    Parameters:\n",
    "        - dgl_graph: Grafo de DGL.\n",
    "        - test_size: Proporción del conjunto de prueba.\n",
    "\n",
    "    Returns:\n",
    "        - (train_pos_u, train_pos_v): Aristas positivas de entrenamiento.\n",
    "        - (test_pos_u, test_pos_v): Aristas positivas de prueba.\n",
    "        - (train_neg_u, train_neg_v): Aristas negativas de entrenamiento.\n",
    "        - (test_neg_u, test_neg_v): Aristas negativas de prueba.\n",
    "        - train_g: Subgrafo de entrenamiento.\n",
    "    \"\"\"\n",
    "    # Obtener las aristas del grafo\n",
    "    u, v = dgl_graph.edges()\n",
    "\n",
    "    # Crear la matriz de adyacencia dispersa\n",
    "    adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())), shape=(dgl_graph.num_nodes(), dgl_graph.num_nodes()))\n",
    "\n",
    "    # Crear la matriz de adyacencia negativa dispersa\n",
    "    adj_neg = sp.coo_matrix(1 - adj.toarray() - sp.eye(dgl_graph.num_nodes()))\n",
    "\n",
    "    # IDs de las aristas negativas\n",
    "    neg_u, neg_v = adj_neg.nonzero()\n",
    "\n",
    "    # Dividir las aristas positivas en entrenamiento y prueba\n",
    "    num_test = int(len(u) * test_size)\n",
    "    num_train = len(u) - num_test\n",
    "\n",
    "    perm = np.random.permutation(len(u))\n",
    "    train_pos_u, train_pos_v = u[perm[:num_train]], v[perm[:num_train]]\n",
    "    test_pos_u, test_pos_v = u[perm[num_train:]], v[perm[num_train:]]\n",
    "\n",
    "    # Dividir las aristas negativas en entrenamiento y prueba\n",
    "    perm_neg = np.random.permutation(len(neg_u))\n",
    "    train_neg_u, train_neg_v = neg_u[perm_neg[:num_train]], neg_v[perm_neg[:num_train]]\n",
    "    test_neg_u, test_neg_v = neg_u[perm_neg[num_train:num_train + num_test]], neg_v[perm_neg[num_train:num_train + num_test]]\n",
    "\n",
    "    # Crear el subgrafo de entrenamiento\n",
    "    train_g = dgl.remove_edges(dgl_graph, perm[num_train:])\n",
    "\n",
    "    return (train_pos_u, train_pos_v), (test_pos_u, test_pos_v), (train_neg_u, train_neg_v), (test_neg_u, test_neg_v), train_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 41.0 GiB for an array with shape (74145, 74145) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m force_reload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dgl_graph \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mCSVDataset(data_path, force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m (train_pos_u, train_pos_v) , (test_pos_u, test_pos_v), (train_neg_u, train_neg_v), (test_neg_u, test_neg_v), train_g \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data_link_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdgl_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36msplit_data_link_pred\u001b[0;34m(dgl_graph, test_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m adj \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcoo_matrix((np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(u)), (u\u001b[38;5;241m.\u001b[39mnumpy(), v\u001b[38;5;241m.\u001b[39mnumpy())), shape\u001b[38;5;241m=\u001b[39m(dgl_graph\u001b[38;5;241m.\u001b[39mnum_nodes(), dgl_graph\u001b[38;5;241m.\u001b[39mnum_nodes()))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Crear la matriz de adyacencia negativa dispersa\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m adj_neg \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcoo_matrix(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m sp\u001b[38;5;241m.\u001b[39meye(dgl_graph\u001b[38;5;241m.\u001b[39mnum_nodes()))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# IDs de las aristas negativas\u001b[39;00m\n\u001b[1;32m     30\u001b[0m neg_u, neg_v \u001b[38;5;241m=\u001b[39m adj_neg\u001b[38;5;241m.\u001b[39mnonzero()\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/scipy/sparse/_coo.py:288\u001b[0m, in \u001b[0;36m_coo_base.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtoarray\u001b[39m(\u001b[38;5;28mself\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 288\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     fortran \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(B\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fortran \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m B\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous:\n",
      "File \u001b[0;32m~/Desktop/GIT/TrabajoTesis/env310/lib/python3.10/site-packages/scipy/sparse/_base.py:1367\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 41.0 GiB for an array with shape (74145, 74145) and data type float64"
     ]
    }
   ],
   "source": [
    "data_path = 'data/DGL_Graph/DiGraph_AllFeatures/'\n",
    "force_reload = True\n",
    "dgl_graph = dgl.data.CSVDataset(data_path, force_reload=False)[0]\n",
    "\n",
    "(train_pos_u, train_pos_v) , (test_pos_u, test_pos_v), (train_neg_u, train_neg_v), (test_neg_u, test_neg_v), train_g = split_data_link_pred(dgl_graph, test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving data into cached files.\n",
      "Graph(num_nodes=74145, num_edges=461889,\n",
      "      ndata_schemes={'feat': Scheme(shape=(72,), dtype=torch.float32)}\n",
      "      edata_schemes={'Relationship': Scheme(shape=(), dtype=torch.int64)})\n",
      "Training edges: 369389\n",
      "Validation edges: 46486\n",
      "Test edges: 46014\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar el dataset Cora\n",
    "data = CoraGraphDataset()\n",
    "g = data[0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "g = g.to(device)\n",
    "\n",
    "# Agregar atributos de nodos y aristas\n",
    "g.ndata['feat'] = g.ndata['feat'].to(device)\n",
    "\n",
    "# Dividir datos en entrenamiento, validación y prueba\n",
    "def split_edges(g, val_ratio=0.05, test_ratio=0.1):\n",
    "    u, v = g.edges()\n",
    "    eids = torch.randperm(g.number_of_edges())\n",
    "    num_val = int(len(eids) * val_ratio)\n",
    "    num_test = int(len(eids) * test_ratio)\n",
    "    num_train = len(eids) - num_val - num_test\n",
    "    \n",
    "    train_eids = eids[:num_train]\n",
    "    val_eids = eids[num_train:num_train+num_val]\n",
    "    test_eids = eids[num_train+num_val:]\n",
    "    \n",
    "    train_g = dgl.remove_edges(g, val_eids.tolist() + test_eids.tolist())\n",
    "    \n",
    "    return train_g, (u[val_eids], v[val_eids]), (u[test_eids], v[test_eids])\n",
    "\n",
    "train_g, val_edges, test_edges = split_edges(g)\n",
    "\n",
    "\n",
    "model = GCN(g.ndata['feat'].shape[1], 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = model.encode(train_g, train_g.ndata['feat'])\n",
    "    \n",
    "    neg_u, neg_v = negative_sampler.GlobalUniform()(train_g, len(val_edges[0]))\n",
    "    \n",
    "#     edge_index = torch.cat([\n",
    "#         torch.stack(val_edges, dim=0),\n",
    "#         torch.stack([neg_u, neg_v], dim=0)\n",
    "#     ], dim=-1)\n",
    "    \n",
    "#     edge_labels = torch.cat([\n",
    "#         torch.ones(val_edges[0].shape[0], device=device),\n",
    "#         torch.zeros(neg_u.shape[0], device=device)\n",
    "#     ])\n",
    "    \n",
    "#     out = model.decode(z, edge_index).view(-1)\n",
    "#     loss = criterion(out, edge_labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "# def test(edges):\n",
    "#     model.eval()\n",
    "#     z = model.encode(g, g.ndata['feat'])\n",
    "#     out = model.decode(z, torch.stack(edges, dim=0)).view(-1).sigmoid()\n",
    "#     return roc_auc_score(torch.ones(len(edges[0]), device=device).cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "# best_val_auc, final_test_auc = 0, 0\n",
    "\n",
    "# for epoch in range(1, 101):\n",
    "#     loss = train()\n",
    "#     val_auc = test(val_edges)\n",
    "#     test_auc = test(test_edges)\n",
    "#     if val_auc > best_val_auc:\n",
    "#         best_val_auc = val_auc\n",
    "#         final_test_auc = test_auc\n",
    "#     print(f'Epoch {epoch}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# print(f'Final Test AUC: {final_test_auc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
